"""
GRPO PyTorch Lightningæ¨¡å—
å°†GRPOè®­ç»ƒç®—æ³•åŒ…è£…åˆ°PyTorch Lightningæ¡†æ¶ä¸­
"""

import torch
import torch.nn as nn
import pytorch_lightning as pl
from typing import Dict, List, Tuple, Optional, Callable, Any
from omegaconf import DictConfig, OmegaConf
import time
import os

try:
    import swanlab
except ImportError:
    swanlab = None

from grpo_trainer import GRPOTrainer
from grpo_rewards import create_reward_function
from graph_discrete_flow_model import GraphDiscreteFlowModel
import utils

class GRPOLightningModule(pl.LightningModule):
    """
    GRPOçš„PyTorch Lightningå®ç°
    """
    
    def __init__(
        self,
        cfg: DictConfig,
        datamodule,
        model_kwargs,
        total_steps: int,
        **kwargs  # Absorb other potential kwargs
    ):
        super().__init__()
        # å°† `cfg` ä¸­çš„æ‰€æœ‰å‚æ•°æå‡åˆ° hparams çš„é¡¶å±‚
        self.save_hyperparameters(cfg)

        # å°†ä¸è®­ç»ƒè¿‡ç¨‹ç›¸å…³çš„å‚æ•°ä¿å­˜ä¸ºå¸¸è§„å®ä¾‹å±æ€§
        self.cfg = cfg
        self.datamodule = datamodule
        self.model_kwargs = model_kwargs
        self.total_steps = total_steps 

        # å®é™…æ¨¡å‹æ˜¯ä¸€ä¸ª GraphDiscreteFlowModel
        self.model = GraphDiscreteFlowModel(cfg=cfg, **model_kwargs)

        # è¿™äº›å°†åœ¨ setup() ä¸­è¢«åˆå§‹åŒ–
        self.reward_function = None
        self.grpo_trainer = None
        # ç”¨äºåœ¨ on_load_checkpoint å’Œ setup ä¹‹é—´ä¼ é€’çŠ¶æ€
        self._grpo_trainer_state_to_restore = None

        print("ğŸš€ GRPO Lightningæ¨¡å—åˆå§‹åŒ–å®Œæˆ (ç­‰å¾… setup é˜¶æ®µæ¥åˆ›å»ºè®­ç»ƒå™¨)")
    
    @staticmethod
    def _get_group_indices_for_rank(num_groups: int, world_size: int, rank: int) -> List[int]:
        """
        è®¡ç®—å½“å‰rankéœ€è¦å¤„ç†çš„groupç´¢å¼•åˆ—è¡¨ï¼Œå®ç°å‡åŒ€åˆ†é…ã€‚

        ä¾‹å¦‚: 8ä¸ªgroup, 3ä¸ªGPU (world_size=3)
        - rank 0: [0, 1, 2] (3ä¸ª)
        - rank 1: [3, 4, 5] (3ä¸ª)
        - rank 2: [6, 7]   (2ä¸ª)

        Args:
            num_groups: æ€»groupæ•°é‡ã€‚
            world_size: åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„æ€»è¿›ç¨‹æ•° (GPUæ•°é‡)ã€‚
            rank: å½“å‰è¿›ç¨‹çš„rankã€‚

        Returns:
            ä¸€ä¸ªåŒ…å«è¯¥rankåº”å¤„ç†çš„groupç´¢å¼•çš„åˆ—è¡¨ã€‚
        """
        if rank >= num_groups:
            # å¦‚æœGPUæ•°é‡å¤šäºgroupæ•°é‡ï¼Œä¸€äº›GPUå°†æ²¡æœ‰ä»»åŠ¡
            return []

        base_groups_per_gpu = num_groups // world_size
        remainder = num_groups % world_size

        # å‰ `remainder` ä¸ªGPUä¼šå¤šåˆ†é…ä¸€ä¸ªgroup
        if rank < remainder:
            num_groups_for_this_rank = base_groups_per_gpu + 1
            start_index = rank * num_groups_for_this_rank
        else:
            num_groups_for_this_rank = base_groups_per_gpu
            # è®¡ç®—èµ·å§‹ç´¢å¼•æ—¶è¦è€ƒè™‘å‰é¢ `remainder` ä¸ªGPUå¤šåˆ†é…çš„éƒ¨åˆ†
            start_index = remainder * (base_groups_per_gpu + 1) + (rank - remainder) * base_groups_per_gpu
        
        end_index = start_index + num_groups_for_this_rank
        return list(range(start_index, end_index))
    
    def setup(self, stage: str) -> None:
        """åœ¨ fit, validate, test, or predict å¼€å§‹æ—¶è°ƒç”¨."""
        if stage == "fit":
            print("ğŸ”§ åœ¨Lightning setupé˜¶æ®µåˆå§‹åŒ–GRPOç»„ä»¶...")
            
            # 1. åˆ›å»ºå¥–åŠ±å‡½æ•°
            # å‡†å¤‡å‚è€ƒæŒ‡æ ‡ï¼ˆå¦‚æœæ¨¡å‹æœ‰çš„è¯ï¼‰
            ref_metrics = None
            if hasattr(self.model, 'dataset_info') and hasattr(self.model.dataset_info, 'ref_metrics'):
                ref_metrics = self.model.dataset_info.ref_metrics
            
            self.reward_function = create_reward_function(
                reward_type=self.cfg.grpo.reward_type,
                cfg=self.cfg,
                device=self.device,
                # ä¼ é€’é¢å¤–å‚æ•°ä»¥å®ç°å‘åå…¼å®¹
                datamodule=self.datamodule,
                model=self.model,
                ref_metrics=ref_metrics,
                name=f"grpo_{self.cfg.grpo.reward_type}"
            )
            
            # 2. åˆå§‹åŒ– GRPO è®­ç»ƒå™¨ï¼Œæ­¤æ—¶ä½¿ç”¨æœªåŒ…è£…çš„æ¨¡å‹
            self.grpo_trainer = GRPOTrainer(
                model=self.model,
                reward_function=self.reward_function,
                cfg=self.cfg,
                model_kwargs=self.model_kwargs,
            )
 
            # 3. å¦‚æœæ˜¯ä»GRPOçš„checkpointæ¢å¤ï¼Œåˆ™æ¢å¤å…¶çŠ¶æ€
            if self._grpo_trainer_state_to_restore:
                print("ğŸ”„ æ­£åœ¨æ¢å¤GRPOè®­ç»ƒå™¨çŠ¶æ€...")
                self.grpo_trainer.load_state_dict(self._grpo_trainer_state_to_restore)
                self._grpo_trainer_state_to_restore = None  # æ¸…ç†çŠ¶æ€

    def on_train_start(self) -> None:
        """
        åœ¨è®­ç»ƒå¼€å§‹æ—¶è°ƒç”¨ï¼Œæ­¤æ—¶æ¨¡å‹å·²è¢«DDPåŒ…è£…ã€‚
        è¿™æ˜¯æ›´æ–°GRPOTrainerå†…éƒ¨æ¨¡å‹å¼•ç”¨å¹¶åˆ›å»ºå‚è€ƒæ¨¡å‹çš„æœ€ä½³æ—¶æœºã€‚
        """
        if self.grpo_trainer:
            # for n,p in self.named_parameters():
            #     if p.requires_grad:
            #         print(n, p.numel())
            # print('æ€»å¯è®­ç»ƒå‚æ•°æ•°ç›®', sum(p.numel() for p in self.parameters() if p.requires_grad))
            
            print(f"ğŸ”§ [on_train_start] æ›´æ–°GRPOTrainerçš„æ¨¡å‹å¼•ç”¨ -> {type(self).__name__}")
            
            # 1. GRPOTrainer.model åº”è¯¥å¼•ç”¨DDPåŒ…è£…åçš„å®Œæ•´æ¨¡å‹
            #    DDPä¸åƒFSDPé‚£æ ·åˆ†ç‰‡å‚æ•°ï¼Œæ‰€ä»¥ç»“æ„æ›´ç®€å•
            self.grpo_trainer.model = self

            # 2. GRPOTrainer.core_model åº”è¯¥å¼•ç”¨åŸå§‹çš„ GraphDiscreteFlowModel
            #    è¯¥å®ä¾‹ä½äº GRPOLightningModule (self) ä¸Šï¼Œç”¨äºå†…éƒ¨ç®—æ³•é€»è¾‘ã€‚
            self.grpo_trainer.core_model = self.model
            print(f"   -> GRPOTrainer.core_model å·²æŒ‡å‘: {type(self.grpo_trainer.core_model).__name__}")
            
            # 3. åœ¨æ­¤é¢„å…ˆåˆ›å»ºå‚è€ƒæ¨¡å‹ï¼Œé¿å…åœ¨è®­ç»ƒæ­¥éª¤ä¸­å‘ç”Ÿå†²çª
            print("ğŸ”§ [on_train_start] é¢„å…ˆåˆ›å»ºå‚è€ƒæ¨¡å‹...")
            self.grpo_trainer._ensure_reference_model()
            print("âœ… [on_train_start] å‚è€ƒæ¨¡å‹å·²æˆåŠŸåˆ›å»ºã€‚")
            
            # 4. åˆå§‹åŒ–KLæƒ©ç½šæ‰€éœ€çš„åŸå§‹æ¨¡å‹çŠ¶æ€
            #    DDPä¸‹æ¯ä¸ªrankéƒ½æœ‰å®Œæ•´çš„å‚æ•°å‰¯æœ¬ï¼Œç›´æ¥ä¿å­˜å³å¯
            if self.grpo_trainer.original_model_state is None:
                print("ğŸ”§ [on_train_start] åˆå§‹åŒ–KLæƒ©ç½šçš„åŸå§‹æ¨¡å‹çŠ¶æ€...")
                
                # ç›´æ¥ä¿å­˜å®Œæ•´çš„å‚æ•°çŠ¶æ€ï¼ŒDDPç¡®ä¿æ¯ä¸ªrankéƒ½æœ‰ç›¸åŒçš„å‚æ•°
                params_found = {
                    name: param.clone().detach()
                    for name, param in self.named_parameters()
                    if param.requires_grad
                }
                self.grpo_trainer.original_model_state = params_found
                        # ğŸ’¡ã€è°ƒè¯•ã€‘æ‰“å°æ¨¡å‹ä¸­æ‰€æœ‰å¯è®­ç»ƒå‚æ•°çš„åç§°
  
    def configure_optimizers(self):
        print("ğŸ”§ [configure_optimizers] æ­£åœ¨é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨...")
        
        if not hasattr(self, 'grpo_trainer') or self.grpo_trainer is None:
            raise RuntimeError("GRPOTrainer å¿…é¡»åœ¨ configure_optimizers ä¹‹å‰è¢«åˆå§‹åŒ–ã€‚")

        target_model = self.grpo_trainer.core_model
        trainable_params = list(target_model.parameters())

        if not trainable_params:
            raise ValueError("[configure_optimizers] é”™è¯¯: ç›®æ ‡æ¨¡å‹ä¸­æœªæ‰¾åˆ°ä»»ä½•å‚æ•°!")

        print(f"âœ… [configure_optimizers] æˆåŠŸä» GRPOTrainer.core_model ä¸­æ‰¾åˆ° {len(trainable_params)} ä¸ªå‚æ•°ã€‚")

        # 1. åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            trainable_params, 
            lr=self.hparams.grpo.learning_rate,
            weight_decay=1e-4
        )
        print("âœ… ä¼˜åŒ–å™¨å·²æˆåŠŸåˆ›å»ºã€‚")

        # 2. åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ (å¸¦é¢„çƒ­)
        warmup_steps = self.hparams.grpo.get('warmup_steps', 0)
        
        if warmup_steps > 0:
            print(f"ğŸ”¥ é…ç½®å­¦ä¹ ç‡é¢„çƒ­: {warmup_steps} æ­¥")
            
            def lr_lambda(current_step):
                if current_step < warmup_steps:
                    return float(current_step) / float(max(1, warmup_steps))
                return 1.0

            scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
            
            print("âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨å·²æˆåŠŸåˆ›å»ºã€‚")
            return {
                "optimizer": optimizer,
                "lr_scheduler": {
                    "scheduler": scheduler,
                    "interval": "step",  # æ¯ä¸ªè®­ç»ƒæ­¥éƒ½æ›´æ–°å­¦ä¹ ç‡
                    "frequency": 1,
                },
            }
        else:
            print("âœ… æœªé…ç½®å­¦ä¹ ç‡é¢„çƒ­ã€‚")
            return optimizer

    def configure_gradient_clipping(self, optimizer, optimizer_idx=None, gradient_clip_val=0.0, gradient_clip_algorithm="value"):
        """é‡å†™é»˜è®¤æ¢¯åº¦è£å‰ªé€»è¾‘ï¼Œå½“æ¢¯åº¦å…¨éƒ¨ä¸º None æ—¶å®‰å…¨åœ°è·³è¿‡è£å‰ªã€‚

        å…¼å®¹DDPç­–ç•¥ï¼šè§£å†³åœ¨é¦–æ¬¡è®­ç»ƒæ­¥éª¤ total_loss ä¸º 0 å¯¼è‡´æ‰€æœ‰å‚æ•°æ— æ¢¯åº¦æ—¶ï¼Œ
        PyTorch å†…éƒ¨ clip_grad_* è°ƒç”¨ _group_tensors_by_device_and_dtype
        æŠ¥é”™ `Expected !nested_tensorlist[0].empty() to be true, but got false.` çš„é—®é¢˜ã€‚
        """

        # è‹¥æœªè®¾ç½®è£å‰ªæˆ–è£å‰ªå€¼ä¸º 0ï¼Œåˆ™ç›´æ¥è·³è¿‡
        if gradient_clip_val is None or gradient_clip_val == 0.0:
            return

        # æ£€æŸ¥æ˜¯å¦è‡³å°‘å­˜åœ¨ä¸€ä¸ªéç©ºæ¢¯åº¦ï¼ˆDDPæ¨¡å¼ä¸‹ç›´æ¥æ£€æŸ¥selfçš„å‚æ•°ï¼‰
        has_any_grad = any(p.grad is not None for p in self.parameters())

        if not has_any_grad:
            # å…¨éƒ¨æ¢¯åº¦ä¸ºç©ºï¼Œç›´æ¥è¿”å›ï¼Œé¿å…è§¦å‘å†…éƒ¨æ–­è¨€é”™è¯¯
            return

        # è°ƒç”¨ Lightning æä¾›çš„ clip_gradients å·¥å…·æ‰§è¡Œå®é™…è£å‰ª
        self.clip_gradients(
            optimizer,
            gradient_clip_val=gradient_clip_val,
            gradient_clip_algorithm=gradient_clip_algorithm,
        )

    def training_step(self, batch, batch_idx):
            """
            æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤ - å®Œæ•´æœ€ç»ˆç‰ˆ (å·²é€‚é…ç†µå¥–åŠ±)
            - ä¸ºæ¯ä¸ªGPUåˆ†é…å¤šä¸ªgroupè¿›è¡Œå¤„ç†ã€‚
            - æ”¶é›†æ¯ä¸ªgroupçš„è¯¦ç»†æ€§èƒ½å’Œå¥–åŠ±æŒ‡æ ‡ã€‚
            - åœ¨æ‰€æœ‰GPUé—´åŒæ­¥æ•°æ® (åŒ…æ‹¬ç†µè®¡ç®—æ‰€éœ€æ•°æ®)ï¼Œè®¡ç®—å…¨å±€æŸå¤±ã€‚
            - åªåœ¨ä¸»è¿›ç¨‹(Rank 0)ä¸Šæ‰§è¡Œæ—¥å¿—è®°å½•å’Œæ‰“å°ã€‚
            """
            world_size = self.trainer.world_size
            global_rank = self.trainer.global_rank
            num_groups = self.cfg.grpo.num_groups

            # 1. åŠ¨æ€è®¡ç®—å½“å‰GPUéœ€è¦å¤„ç†çš„groupç´¢å¼•
            group_indices_for_this_rank = self._get_group_indices_for_rank(num_groups, world_size, global_rank)
            
            if not group_indices_for_this_rank:
                # å¦‚æœGPUæ•°é‡å¤šäºgroupæ•°é‡ï¼ŒæŸäº›GPUå¯èƒ½æ²¡æœ‰ä»»åŠ¡
                return None

            # 2. åœ¨å½“å‰GPUä¸Šå¤„ç†åˆ†é…åˆ°çš„æ¯ä¸€ä¸ªgroupï¼Œå¹¶æ”¶é›†æœ¬åœ°æ•°æ®
            local_rewards_list = []
            local_current_log_probs_list = []
            local_ref_log_probs_list = []
            # ğŸ’¡ æ–°å¢: æ”¶é›†ç†µè®¡ç®—æ‰€éœ€çš„æ•°æ®
            local_model_preds_list = []
            local_node_masks_list = []
            local_metrics_to_log = {}

            try:
                # è¿›å…¥ .eval() æ¨¡å¼è¿›è¡Œé‡‡æ ·ï¼Œä»¥è·å¾—ç¡®å®šçš„ã€å¯å¤ç°çš„ç­–ç•¥è¯„ä¼°
                original_mode = self.model.training
                self.model.eval()

                for group_idx in group_indices_for_this_rank:
                    # ğŸ’¡ ä¿®æ”¹: æ¥æ”¶æ‰€æœ‰è¿”å›çš„æ•°æ®ï¼ŒåŒ…æ‹¬ model_pred å’Œ node_mask
                    (groups, rewards, cumulative_loss, 
                    current_log_probs, reference_log_probs, 
                    model_pred, node_mask,
                    group_metrics) = self.grpo_trainer.sample_and_compute_single_group(
                        global_rank=global_rank
                    )
                    # æ·»åŠ åˆ°å„è‡ªçš„åˆ—è¡¨ä¸­
                    local_rewards_list.append(rewards)
                    local_current_log_probs_list.append(current_log_probs)
                    local_ref_log_probs_list.append(reference_log_probs)
                    # ğŸ’¡ æ–°å¢: æ”¶é›†æ–°æ•°æ®
                    local_model_preds_list.append(model_pred)
                    local_node_masks_list.append(node_mask)

                    # æ›´æ–°è¦è®°å½•çš„æŒ‡æ ‡
                    local_metrics_to_log.update(group_metrics)

            except Exception as e:
                print(f"âŒ [GPU {global_rank}] GRPOé‡‡æ ·æ­¥éª¤å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return {'loss': torch.tensor(0.0, device=self.device, requires_grad=True)}
            finally:
                # æ— è®ºæˆåŠŸä¸å¦ï¼Œéƒ½æ¢å¤æ¨¡å‹åŸå§‹çš„è®­ç»ƒæ¨¡å¼
                self.model.train(original_mode)


            # 3. åœ¨æ‰€æœ‰GPUé—´åŒæ­¥æ•°æ® (é‡æ„ä»¥æ”¯æŒè‡ªåŠ¨å¾®åˆ†)
            # 3.1. å°†æœ¬åœ°åˆ—è¡¨è¿æ¥æˆå•ä¸ªå¼ é‡
            local_rewards = torch.cat(local_rewards_list) if local_rewards_list else torch.empty(0, device=self.device)
            local_current_log_probs = torch.cat(local_current_log_probs_list) if local_current_log_probs_list else torch.empty(0, device=self.device)
            local_ref_log_probs = torch.cat(local_ref_log_probs_list) if local_ref_log_probs_list else torch.empty(0, device=self.device)
            local_node_masks = torch.cat(local_node_masks_list) if local_node_masks_list else torch.empty(0, device=self.device, dtype=torch.bool)

            # 3.2. ç‰¹åˆ«å¤„ç† model_preds, å®ƒä»¬æ˜¯ PlaceHolder å¯¹è±¡
            if local_model_preds_list and any(p is not None for p in local_model_preds_list):
                local_preds_X = torch.cat([p.X for p in local_model_preds_list])
                local_preds_E = torch.cat([p.E for p in local_model_preds_list])
                valid_y = [p.y for p in local_model_preds_list if p.y is not None]
                local_preds_y = torch.cat(valid_y) if valid_y else torch.empty(0, device=self.device)
            else:
                # å¦‚æœå½“å‰GPUæ²¡æœ‰å¤„ç†ä»»ä½•groupï¼Œåˆ™åˆ›å»ºæ­£ç¡®å½¢çŠ¶çš„ç©ºå¼ é‡
                # æ³¨æ„: è¿™é‡Œçš„ç»´åº¦ä¿¡æ¯éœ€è¦æ ¹æ®ä½ çš„æ¨¡å‹å…·ä½“æƒ…å†µè°ƒæ•´
                bs, n, c = self.cfg.grpo.group_size, self.cfg.grpo.target_node_count, self.model.output_dims['X']
                local_preds_X = torch.empty((0, n, c), device=self.device)
                local_preds_E = torch.empty((0, n, n, self.model.output_dims['E']), device=self.device)
                local_preds_y = torch.empty((0, self.model.output_dims['y']), device=self.device)

            # 3.3. ä½¿ç”¨ all_gather_into_tensor è¿›è¡Œé«˜æ•ˆã€å¯å¾®åˆ†çš„åŒæ­¥
            def gather_autograd_tensor(tensor: torch.Tensor) -> torch.Tensor:
                """ä½¿ç”¨ all_gather_into_tensor åŒæ­¥å¼ é‡å¹¶ä¿ç•™æ¢¯åº¦ã€‚"""
                if not torch.distributed.is_initialized() or self.trainer.world_size == 1:
                    return tensor
                
                # a. è·å–æ‰€æœ‰GPUä¸Šçš„å¼ é‡å¤§å° (ä»…ç¬¬ä¸€ä¸ªç»´åº¦)
                local_size = torch.tensor([tensor.shape[0]], device=tensor.device)
                all_sizes = [torch.zeros_like(local_size) for _ in range(self.trainer.world_size)]
                torch.distributed.all_gather(all_sizes, local_size)
                
                # b. å¦‚æœæ‰€æœ‰å¼ é‡éƒ½ä¸ºç©ºï¼Œåˆ™æ— éœ€æ”¶é›†
                if tensor.shape[0] == 0 and all(s.item() == 0 for s in all_sizes):
                    return tensor

                # c. åˆ›å»ºä¸€ä¸ªè¶³å¤Ÿå¤§çš„è¾“å‡ºå¼ é‡
                total_size = sum(s.item() for s in all_sizes)
                output_shape = (total_size,) + tensor.shape[1:]
                output_tensor = torch.empty(output_shape, dtype=tensor.dtype, device=tensor.device)
                
                # d. æ‰§è¡Œæ”¶é›†
                torch.distributed.all_gather_into_tensor(output_tensor, tensor)
                return output_tensor

            global_rewards = gather_autograd_tensor(local_rewards)
            global_current_log_probs = gather_autograd_tensor(local_current_log_probs)
            global_ref_log_probs = gather_autograd_tensor(local_ref_log_probs)
            global_node_masks = gather_autograd_tensor(local_node_masks)
            global_preds_X = gather_autograd_tensor(local_preds_X)
            global_preds_E = gather_autograd_tensor(local_preds_E)
            global_preds_y = gather_autograd_tensor(local_preds_y)

            # 3.4. é‡æ–°ç»„è£… PlaceHolder å¯¹è±¡
            global_model_preds = utils.PlaceHolder(X=global_preds_X, E=global_preds_E, y=global_preds_y)

            # 4. è®¡ç®—GRPOæŸå¤± (ç°åœ¨ä½¿ç”¨å•ä¸ªå…¨å±€å¼ é‡)
            loss_result = self.grpo_trainer.compute_grpo_loss(
                rewards=global_rewards, 
                current_log_probs=global_current_log_probs, 
                reference_log_probs=global_ref_log_probs,
                model_preds=global_model_preds,
                node_masks=global_node_masks,
                global_rank=global_rank
            )
            # å¦‚æœæŸå¤±å‡½æ•°å› ä¸ºæ•°æ®ä¸ºç©ºç­‰åŸå› æ²¡æœ‰è¿”å›æŸå¤±ï¼Œåˆ™æˆ‘ä»¬è·³è¿‡è¿™ä¸ªä¼˜åŒ–æ­¥éª¤
            if "total_loss" not in loss_result:
                return None
                
            loss = loss_result["total_loss"]
            
            # 5. æ›´æ–°å‚è€ƒæ¨¡å‹ (æ‰€æœ‰è¿›ç¨‹éƒ½éœ€è¦æ‰§è¡Œä»¥ä¿æŒåŒæ­¥)
            # ğŸ’¡ å»ºè®®: å°† reference_update_frequency ä¹Ÿæ”¾å…¥é…ç½®æ–‡ä»¶ä¸­
            update_freq = getattr(self.cfg.grpo, 'ref_model_update_freq', 200)
            self.grpo_trainer._update_reference_model(update_frequency=update_freq)

            # 6. SwanLabæ—¥å¿—è®°å½• (åªåœ¨ä¸»è¿›ç¨‹ Rank 0 ä¸Šæ‰§è¡Œ)
            if self.trainer.is_global_zero: # ä½¿ç”¨ Pytorch Lightning çš„æ¨èæ–¹å¼
                # 6.1 æ”¶é›†å¹¶èšåˆæ‰€æœ‰GPUä¸Šçš„è¯¦ç»†æ€§èƒ½æŒ‡æ ‡
                all_gpu_metrics = [None for _ in range(world_size)]
                torch.distributed.all_gather_object(all_gpu_metrics, local_metrics_to_log)
                
                final_metrics_to_log = {}
                for metrics_dict in all_gpu_metrics:
                    if metrics_dict:
                        final_metrics_to_log.update(metrics_dict)
                
                # 6.2 åˆå¹¶æŸå¤±è®¡ç®—è¿”å›çš„å…¨å±€æŒ‡æ ‡
                if "metrics" in loss_result:
                    final_metrics_to_log.update(loss_result["metrics"])
                
                # 6.3 è®°å½•å­¦ä¹ ç‡
                final_metrics_to_log['learning_rate'] = self.optimizers().param_groups[0]['lr']
                
                # 6.4 ä¸€æ¬¡æ€§è®°å½•æ‰€æœ‰æŒ‡æ ‡
                if swanlab is not None and swanlab.run is not None:
                    swanlab.log(final_metrics_to_log, step=self.grpo_trainer.global_step)
                else:
                    print("Swanlab logger failed.")
            # 7. æ›´æ–°å…¨å±€æ­¥æ•° (æ‰€æœ‰è¿›ç¨‹éƒ½éœ€è¦çŸ¥é“ï¼Œä»¥ä¾¿åŒæ­¥å‚è€ƒæ¨¡å‹æ›´æ–°)
            self.grpo_trainer.global_step += 1
            
            return loss
    
    def on_save_checkpoint(self, checkpoint: Dict[str, Any]) -> None:
        """
        ä¿å­˜checkpointæ—¶çš„å›è°ƒã€‚æ·»åŠ GRPOç‰¹å®šçš„çŠ¶æ€ã€‚
        """
        if self.grpo_trainer:
            # ä½¿ç”¨æ–°çš„ state_dict æ–¹æ³•
            checkpoint["grpo_trainer_state"] = self.grpo_trainer.state_dict()
            global_step = self.grpo_trainer.global_step
            print(f"ğŸ’¾ ä¿å­˜GRPO checkpoint, å…¨å±€æ­¥æ•°: {global_step}")
        else:
            print("âš ï¸ è­¦å‘Š: on_save_checkpointè¢«è°ƒç”¨ï¼Œä½†grpo_traineræœªåˆå§‹åŒ–ã€‚")

    def on_train_epoch_end(self):
        """è®­ç»ƒå‘¨æœŸç»“æŸæ—¶çš„å›è°ƒ"""
        # ç¡®ä¿ grpo_trainer å­˜åœ¨
        if hasattr(self, 'grpo_trainer') and self.grpo_trainer:
            current_step = self.grpo_trainer.global_step
            print(f"ğŸ“Š Epoch {self.current_epoch} ç»“æŸ, å…¨å±€æ­¥æ•°: {current_step}")
        else:
            print(f"ğŸ“Š Epoch {self.current_epoch} ç»“æŸ (GRPOè®­ç»ƒå™¨æœªåˆå§‹åŒ–)")
    
    def validation_step(self, batch, batch_idx):
        """éªŒè¯æ­¥éª¤ - åœ¨GRPOä¸­é€šå¸¸è·³è¿‡"""
        pass
    
    def get_graph_model(self):
        """è·å–åº•å±‚çš„å›¾ç”Ÿæˆæ¨¡å‹"""
        return self.model
    
    def sample_graphs(self, batch_size: int = None, **kwargs):
        """é‡‡æ ·å›¾çš„ä¾¿æ·æ–¹æ³•"""
        if batch_size is None:
            batch_size = self.cfg.grpo.group_size
        
        return self.grpo_trainer.sample_graphs_with_gradients(batch_size=batch_size, **kwargs)
    
    @torch.no_grad()
    def validate_pure_sampling(self, batch_size: int = 8, seed: Optional[int] = 42, save_samples: bool = True):
        """
        çº¯é‡‡æ ·éªŒè¯ï¼šç›´æ¥è°ƒç”¨åº•å±‚çš„ GraphDiscreteFlowModel.sample_batch æ–¹æ³•
        å¹¶ä½¿ç”¨åŸå§‹çš„ sampling_metrics è¿›è¡Œè¯„ä¼°ï¼Œä»¥éªŒè¯ checkpoint çš„çœŸå®è´¨é‡ã€‚
        """
        print("ğŸ” å¼€å§‹çº¯é‡‡æ ·éªŒè¯ (è°ƒç”¨åŸå§‹ sample_batch)...")
        self.model.eval()

        # 1. ç›´æ¥è°ƒç”¨åŸå§‹çš„ã€ç»è¿‡éªŒè¯çš„é‡‡æ ·æ–¹æ³•
        print(f"   è°ƒç”¨ self.model.sample_batch with batch_size={batch_size}, seed={seed}")
        sampled_graphs, sampled_labels = self.model.sample_batch(
            batch_id=seed,  # ä½¿ç”¨ç§å­ä½œä¸ºæ‰¹æ¬¡IDä»¥ç¡®ä¿å¯å¤ç°
            batch_size=batch_size,
            num_nodes=self.cfg.grpo.target_node_count,
            save_final=batch_size if save_samples else 0,
            keep_chain=0,
            number_chain_steps=self.cfg.sample.sample_steps,
            save_visualization=save_samples,
        )
        print(f"   âœ… æˆåŠŸä»åŸå§‹ sample_batch ç”Ÿæˆ {len(sampled_graphs)} ä¸ªå›¾")

        # 2. åˆ‡æ¢åˆ°ä½¿ç”¨åŸå§‹çš„ sampling_metrics è¿›è¡Œè¯„ä¼°
        print("   ğŸ“Š ä½¿ç”¨åŸå§‹ sampling_metrics è¿›è¡Œè´¨é‡è¯„ä¼°...")
        
        if not hasattr(self.model, 'sampling_metrics') or self.model.sampling_metrics is None:
            print("   âš ï¸ åŸå§‹ sampling_metrics æœªåˆå§‹åŒ–ï¼Œè·³è¿‡è¯„ä¼°")
            return {'status': 'sampled_only', 'num_samples': len(sampled_graphs)}

        try:
            # è°ƒç”¨åŸå§‹çš„è¯„ä¼°å‡½æ•°ï¼Œå®ƒä¼šè‡ªå·±æ‰“å°è¯¦ç»†çš„MMDåˆ†æ•°
            quality_metrics = self.model.sampling_metrics(
                sampled_graphs, # ç›´æ¥ä½œä¸ºä½ç½®å‚æ•°ä¼ é€’
                ref_metrics=self.model.dataset_info.ref_metrics,
                name=self.cfg.general.name,
                current_epoch=0,  # ç¡¬ç¼–ç ï¼Œå› ä¸ºæ²¡æœ‰ trainer
                val_counter=-1,
                test=True, # æ ‡è®°ä¸ºæµ‹è¯•æ¨¡å¼
                local_rank=0, # ç¡¬ç¼–ç ï¼Œå› ä¸ºæ²¡æœ‰ trainer
                labels=sampled_labels if self.model.conditional else None,
            )
            
            # sampling_metrics å¯¹è±¡ä¼šè‡ªå·±æ‰“å°è¯¦ç»†æ—¥å¿—
            print(f"   ğŸ“ˆ åŸå§‹æŒ‡æ ‡è¯„ä¼°å®Œæˆã€‚è¯·æŸ¥çœ‹ä¸Šæ–¹ç”± 'sampling_metrics' æ‰“å°çš„è¯¦ç»†MMDåˆ†æ•°ã€‚")

            # å…¼å®¹ä¹‹å‰çš„è¿”å›æ ¼å¼ï¼Œè¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
            return quality_metrics

        except Exception as e:
            import traceback
            print(f"   âŒ åŸå§‹ sampling_metrics è¯„ä¼°å¤±è´¥: {e}")
            traceback.print_exc()
            return {'error': str(e)}

    def forward(self, *args, **kwargs):
        """å‰å‘ä¼ æ’­ - å§”æ‰˜ç»™å›¾æ¨¡å‹"""
        return self.model(*args, **kwargs)


class DummyDataModule(pl.LightningDataModule):
    """
    è™šæ‹Ÿæ•°æ®æ¨¡å— - ä¸“ä¸ºGRPOå¤šå¡è®­ç»ƒè®¾è®¡
    
    GRPOä¸éœ€è¦çœŸå®çš„æ•°æ®åŠ è½½å™¨ï¼Œå› ä¸ºå›¾æ˜¯ä»æ¨¡å‹ä¸­é‡‡æ ·çš„ã€‚
    è¿™ä¸ªç±»æä¾›çš„æ•°æ®åŠ è½½å™¨ä¼šç”Ÿæˆä¸num_groupsç›¸ç­‰çš„è™šæ‹Ÿæ•°æ®ï¼Œ
    ç¡®ä¿DDPèƒ½å¤Ÿæ­£ç¡®åœ°å°†ä¸åŒçš„groupåˆ†é…ç»™ä¸åŒçš„GPUã€‚
    """
    
    def __init__(self, num_groups: int = 1, num_workers: int = 0):
        super().__init__()
        self.num_groups = num_groups
        self.num_workers = num_workers
        print(f"ğŸ”§ DummyDataModuleåˆå§‹åŒ–: num_groups={num_groups}")
    
    def setup(self, stage: str = None):
        """è®¾ç½®æ•°æ®"""
        # åˆ›å»ºä¸num_groupsç›¸ç­‰æ•°é‡çš„è™šæ‹Ÿæ•°æ®ï¼Œæ¯ä¸ªæ•°æ®ä»£è¡¨ä¸€ä¸ªgroupçš„ç´¢å¼•
        self.dummy_data = torch.arange(self.num_groups, dtype=torch.float32).unsqueeze(1)  # [num_groups, 1]
        print(f"ğŸ”§ DummyDataModuleè®¾ç½®äº†{len(self.dummy_data)}ä¸ªè™šæ‹Ÿgroup")
    
    def train_dataloader(self):
        """è®­ç»ƒæ•°æ®åŠ è½½å™¨ - æ¯ä¸ªæ•°æ®é¡¹ä»£è¡¨ä¸€ä¸ªgroup"""
        from torch.utils.data import DataLoader, TensorDataset
        dataset = TensorDataset(self.dummy_data)
        return DataLoader(
            dataset,
            batch_size=self.num_groups,  # ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰groups
            shuffle=False,
            num_workers=self.num_workers,
            persistent_workers=False,
        )
    
    def val_dataloader(self):
        """éªŒè¯æ•°æ®åŠ è½½å™¨"""
        return self.train_dataloader()


def create_grpo_lightning_module(
    cfg: DictConfig,
    model_kwargs: dict,
    datamodule,
    total_steps: int,
):
    """
    åˆ›å»ºGRPO Lightningæ¨¡å—
    
    Args:
        cfg: é…ç½®å¯¹è±¡
        model_kwargs: æ¨¡å‹å‚æ•°å­—å…¸
        datamodule: æ•°æ®æ¨¡å—
        total_steps: æ€»è®­ç»ƒæ­¥æ•°
    """
    print("ğŸ”§ æ­£åœ¨åˆ›å»ºGRPO Lightningæ¨¡å—...")
    
    # å®ä¾‹åŒ–GRPOæ¨¡å—
    grpo_module = GRPOLightningModule(
        cfg=cfg,
        datamodule=datamodule,
        model_kwargs=model_kwargs,
        total_steps=total_steps,
    )
    print("ğŸš€ GRPO Lightningæ¨¡å—å®ä¾‹åŒ–å®Œæˆ")
    return grpo_module


def run_grpo_lightning_testing(cfg: DictConfig, checkpoint_path: str):
    """
    ä½¿ç”¨Lightningæ¡†æ¶è¿›è¡ŒGRPOæ¨¡å‹æµ‹è¯•/æ¨ç†
    
    Args:
        cfg: é…ç½®å¯¹è±¡
        checkpoint_path: GRPO checkpointè·¯å¾„
    """
    print(f"ğŸ” å¼€å§‹GRPO Lightningæµ‹è¯•: {checkpoint_path}")
    
    # è®¾ç½®è®¾å¤‡
    use_gpu = cfg.general.gpus > 0 and torch.cuda.is_available()
    device = torch.device("cuda:0" if use_gpu else "cpu")
    
    try:
        # ä»checkpointåŠ è½½GRPOæ¨¡å—
        grpo_module = GRPOLightningModule.load_from_checkpoint(
            checkpoint_path,
            map_location=device,
            strict=False,
        )
        grpo_module.eval()
        
        print("âœ… GRPOæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æ‰§è¡Œæµ‹è¯•é€»è¾‘
        with torch.no_grad():
            # é‡‡æ ·ä¸€äº›å›¾è¿›è¡Œæµ‹è¯•
            test_graphs, _, _, _, _ = grpo_module.sample_graphs(
                batch_size=cfg.grpo.group_size,
            )
            
            print(f"ğŸ“Š æˆåŠŸé‡‡æ · {cfg.grpo.group_size} ä¸ªæµ‹è¯•å›¾")
            
            # è®¡ç®—å¥–åŠ±
            if hasattr(grpo_module, 'reward_function'):
                graph_list = grpo_module.grpo_trainer._convert_to_graph_list(
                    test_graphs, 
                    torch.ones(cfg.grpo.group_size, test_graphs.X.size(1), dtype=torch.bool, device=device)
                )
                rewards = grpo_module.reward_function(graph_list)
                print(f"ğŸ¯ æµ‹è¯•å›¾å¹³å‡å¥–åŠ±: {rewards.mean().item():.4f}")
        
        return grpo_module
        
    except Exception as e:
        print(f"âŒ GRPO Lightningæµ‹è¯•å¤±è´¥: {e}")
        raise e 