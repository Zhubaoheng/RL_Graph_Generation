import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import random
import os
from tqdm import tqdm
from typing import Dict, List, Tuple, Optional, Callable
import pytorch_lightning as pl
from torch.utils.checkpoint import checkpoint as grad_checkpoint

from src import utils
from flow_matching import flow_matching_utils
from grpo_rewards import create_reward_function
from graph_discrete_flow_model import GraphDiscreteFlowModel


class GRPOTrainer:
    """
    Group Relative Policy Optimization (GRPO) è®­ç»ƒå™¨
    
    ç”¨äºå¾®è°ƒå›¾ç”Ÿæˆæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨ã€‚
    é‡‡ç”¨åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œç»“åˆPPOè£å‰ªæœºåˆ¶ã€‚
    """
    
    def __init__(
        self,
        model: pl.LightningModule,
        reward_function: Callable[[List], torch.Tensor],
        cfg: Dict,
        model_kwargs: dict,
    ):
        """
        åˆå§‹åŒ–GRPOè®­ç»ƒå™¨
        
        Args:
            model: é¢„è®­ç»ƒçš„å›¾ç”Ÿæˆæ¨¡å‹ï¼ˆå¯èƒ½è¢«DataParallelåŒ…è£…ï¼‰
            reward_function: å¥–åŠ±å‡½æ•°ï¼Œæ¥æ”¶å›¾åˆ—è¡¨è¿”å›å¥–åŠ±å¼ é‡
            cfg: é…ç½®å­—å…¸
            model_kwargs: åˆ›å»ºæ–°æ¨¡å‹å®ä¾‹æ‰€éœ€çš„å‚æ•°
        """
        self.model = model
        self.reward_function = reward_function
        self.cfg = cfg
        self.model_kwargs = model_kwargs
        grpo_config = cfg.grpo
        
        # ä»é…ç½®ä¸­æå–å‚æ•°
        self.learning_rate = grpo_config.learning_rate
        self.group_size = grpo_config.group_size
        self.num_groups = grpo_config.num_groups
        self.beta = grpo_config.kl_penalty # KLæƒ©ç½šç³»æ•°
        self.clip_ratio = grpo_config.clip_ratio # PPOè£å‰ªæ¯”ä¾‹
        self.gradient_accumulation_steps = grpo_config.gradient_accumulation_steps
        self.ref_model_update_freq = getattr(grpo_config, 'ref_model_update_freq', 200) # å‚è€ƒæ¨¡å‹æ›´æ–°é¢‘ç‡
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¢«DataParallelåŒ…è£…
        self.is_multi_gpu = hasattr(model, 'module')
        
        # è·å–åŸå§‹æ¨¡å‹ï¼ˆå»é™¤DataParallelåŒ…è£…ï¼‰
        self.core_model = model.module if self.is_multi_gpu else model
        
        # èŠ‚ç‚¹æ•°é…ç½®
        self.target_node_count = getattr(grpo_config, 'target_node_count', None)
        self.node_count_range = None
        if hasattr(grpo_config, 'node_count_range') and grpo_config.node_count_range is not None:
            self.node_count_range = tuple(grpo_config.node_count_range)
        elif (hasattr(grpo_config, 'node_count_min') and hasattr(grpo_config, 'node_count_max') and
              grpo_config.node_count_min is not None and grpo_config.node_count_max is not None):
            self.node_count_range = (grpo_config.node_count_min, grpo_config.node_count_max)

        # éªŒè¯èŠ‚ç‚¹æ•°é…ç½®
        self._validate_node_config()
        
        # å­˜å‚¨åŸå§‹æ¨¡å‹å‚æ•°ç”¨äºKLæƒ©ç½š - å°†åœ¨ on_train_start ä¸­åˆå§‹åŒ–
        self.original_model_state = None
        
        # åˆ›å»ºå‚è€ƒç­–ç•¥æ¨¡å‹ç”¨äºé‡è¦æ€§æƒé‡è®¡ç®—
        self.reference_model = None
        # å»¶è¿Ÿåˆ›å»ºreference modelï¼Œç­‰å¾…æ¨¡å‹è®¾å¤‡åˆ†é…å®Œæˆ
        self._reference_model_created = False
        
        # è®­ç»ƒæ­¥æ•°è®¡æ•°å™¨
        self.global_step = 0
        
        print(f"GRPOè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ:")
        print(f" ç»„å¤§å°: {self.group_size}, ç»„æ•°: {self.num_groups}")
        print(f" å­¦ä¹ ç‡: {self.learning_rate}, KLæƒ©ç½š: {self.beta}, PPOè£å‰ª: {self.clip_ratio}")
        print(f" ç›®æ ‡èŠ‚ç‚¹æ•°: {self.target_node_count}, èŠ‚ç‚¹æ•°èŒƒå›´: {self.node_count_range}")
        print(f" å¤šGPUæ¨¡å¼: {self.is_multi_gpu}")

    def _validate_node_config(self):
        """éªŒè¯èŠ‚ç‚¹æ•°é…ç½®"""
        if self.target_node_count is None and self.node_count_range is None:
            raise ValueError(
                "å¿…é¡»æŒ‡å®šèŠ‚ç‚¹æ•°é…ç½®ï¼è¯·è®¾ç½® target_node_count æˆ– node_count_range"
            )

    def _create_reference_model(self):
        """
        åˆ›å»ºå‚è€ƒç­–ç•¥æ¨¡å‹ï¼Œé€‚é…DDPæ¶æ„ã€‚
        DDPæ¨¡å¼ä¸‹ç»“æ„æ›´ç®€å•ï¼Œæ— éœ€å¤æ‚çš„åˆ†ç‰‡å¤„ç†ã€‚
        """
        print("ğŸ”„ åˆ›å»ºå‚è€ƒç­–ç•¥æ¨¡å‹ (DDPæ¨¡å¼)...")
        
        # è·å–ä¸»æ¨¡å‹çš„è®¾å¤‡
        device = next(self.model.parameters()).device
        
        # æ­¥éª¤ 1: åˆ›å»ºä¸€ä¸ªæ–°çš„ã€æ™®é€šçš„æ¨¡å‹å®ä¾‹ï¼Œä½œä¸ºå‚è€ƒæ¨¡å‹çš„â€œå®¹å™¨â€
        # è¿™ä¸ªæ–°æ¨¡å‹åœ¨CPUä¸Šåˆ›å»ºï¼Œä»¥é¿å…å ç”¨GPUæ˜¾å­˜
        self.reference_model = GraphDiscreteFlowModel(cfg=self.cfg, **self.model_kwargs).to('cpu')
        
        # æ­¥éª¤ 2: è·å–DDPæ¨¡å‹çš„çŠ¶æ€å­—å…¸
        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿åœ¨è·å–state_dictå‰æ¨¡å‹å¤„äºevalæ¨¡å¼
        original_training_mode = self.model.training
        self.model.eval()
        
        try:
            with torch.no_grad():
                # DDPæ¨¡å¼ä¸‹ç›´æ¥è·å–state_dictï¼Œæ— éœ€ç‰¹æ®ŠAPI
                full_state_dict = {k: v.clone().detach().cpu() for k, v in self.model.state_dict().items()}
        finally:
            # æ¢å¤åŸå§‹è®­ç»ƒæ¨¡å¼
            self.model.train(original_training_mode)

        # æ­¥éª¤ 3: é‡æ˜ å°„çŠ¶æ€å­—å…¸çš„é”®ä»¥åŒ¹é…å‚è€ƒæ¨¡å‹
        # DDP åŒ…è£… GRPOLightningModule åï¼Œå‚æ•°é”®åå¯èƒ½åŒ…å«:
        # `module.model.model.layer.weight` æˆ– `model.model.layer.weight`
        # è€Œæˆ‘ä»¬çš„å‚è€ƒæ¨¡å‹ (ä¸€ä¸ªæ™®é€šçš„ GraphDiscreteFlowModel) æœŸæœ›çš„é”®åæ˜¯:
        # `model.layer.weight`
        # å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å‰¥ç¦»å¤šä½™çš„å‰ç¼€ã€‚
        print("    é‡æ˜ å°„state_dicté”®åä»¥åŒ¹é…å‚è€ƒæ¨¡å‹ç»“æ„...")
        remapped_state_dict = {}
        
        # DDPå¯èƒ½çš„å‰ç¼€
        possible_prefixes = ["module.model.", "model."]
        
        # æ£€æŸ¥æ”¶åˆ°çš„é”®ï¼Œç¡®å®šä½¿ç”¨å“ªä¸ªå‰ç¼€
        first_key = next(iter(full_state_dict.keys()), "")
        prefix_to_strip = ""
        
        for prefix in possible_prefixes:
            if any(k.startswith(prefix) for k in full_state_dict.keys()):
                prefix_to_strip = prefix
                break
        
        if not prefix_to_strip:
             print(f"    âš ï¸ è­¦å‘Š: åœ¨ state_dict ä¸­æœªæ‰¾åˆ°é¢„æœŸçš„å‰ç¼€ã€‚ç¬¬ä¸€ä¸ªé”®æ˜¯ '{first_key}'ã€‚")
             print(f"    â„¹ï¸ è¿™å¯èƒ½æ„å‘³ç€æ¨¡å‹åŒ…è£…ç»“æ„å·²æ›´æ”¹ã€‚å°†å°è¯•ä¸å‰¥ç¦»å‰ç¼€ã€‚")

        for k, v in full_state_dict.items():
            if prefix_to_strip and k.startswith(prefix_to_strip):
                new_key = k[len(prefix_to_strip):]
                remapped_state_dict[new_key] = v
            else:
                # å¦‚æœæ²¡æœ‰å‰ç¼€ï¼Œä¹Ÿä¿ç•™è¯¥é”®ï¼ˆä¾‹å¦‚ï¼Œéæ¨¡å‹å‚æ•°ï¼‰
                 remapped_state_dict[k] = v
        
        # æ­¥éª¤ 4: å°†æå–å¹¶é‡æ˜ å°„åçš„å®Œæ•´æƒé‡åŠ è½½åˆ°æ–°çš„å‚è€ƒæ¨¡å‹å®ä¾‹ä¸­
        self.reference_model.load_state_dict(remapped_state_dict, strict=True)
        # æ­¥éª¤ 5: å°†å‚è€ƒæ¨¡å‹ç§»åŠ¨åˆ°æ­£ç¡®çš„GPUè®¾å¤‡ä¸Šï¼Œå¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.reference_model = self.reference_model.to(device)
        
        # å†»ç»“å‚è€ƒæ¨¡å‹å‚æ•°
        for param in self.reference_model.parameters():
            param.requires_grad = False
        
        self.reference_model.eval()
        self._reference_model_created = True
        print("âœ… å‚è€ƒç­–ç•¥æ¨¡å‹åˆ›å»ºå®Œæˆ (DDPæ¨¡å¼)")

    def _ensure_reference_model(self):
        """ç¡®ä¿å‚è€ƒæ¨¡å‹å·²åˆ›å»ºï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»º"""
        if not self._reference_model_created:
            self._create_reference_model()
        else:
            # ğŸ”§ æ£€æŸ¥è®¾å¤‡ä¸€è‡´æ€§ï¼Œåœ¨DDPç¯å¢ƒä¸‹è®¾å¤‡å¯èƒ½ä¼šæ”¹å˜
            model_device = next(self.model.parameters()).device
            if self.reference_model is not None:
                ref_device = next(self.reference_model.parameters()).device
                if model_device != ref_device:
                    print(f"ğŸ”§ æ£€æµ‹åˆ°è®¾å¤‡ä¸ä¸€è‡´ï¼Œç§»åŠ¨å‚è€ƒæ¨¡å‹ä» {ref_device} åˆ° {model_device}")
                    self.reference_model = self.reference_model.to(model_device)

    def _update_reference_model(self, update_frequency: int = 1000):
        """
        æ›´æ–°å‚è€ƒç­–ç•¥æ¨¡å‹ï¼Œé€‚é…DDPæ¶æ„ã€‚
        """
        import torch.distributed as dist

        # ç¡®ä¿å‚è€ƒæ¨¡å‹å·²åˆ›å»ºï¼Œè¿™æ˜¯å…ˆå†³æ¡ä»¶
        self._ensure_reference_model()
        
        # åœ¨ç¬¬0æ­¥ä¹‹åæ‰å¼€å§‹æ›´æ–°ï¼Œå› ä¸ºç¬¬0æ­¥æ—¶æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹æ˜¯å®Œå…¨ä¸€æ ·çš„
        if self.global_step > 0 and self.global_step % update_frequency == 0:
            print(f"ğŸ”„ æ›´æ–°å‚è€ƒç­–ç•¥æ¨¡å‹ (step {self.global_step}) (DDPæ¨¡å¼)")
            
            device = next(self.model.parameters()).device
            
            try:
                # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿åœ¨è·å–state_dictå‰æ¨¡å‹å¤„äºevalæ¨¡å¼ï¼Œé¿å…ä¸æ¢¯åº¦è®¡ç®—å†²çª
                original_training_mode = self.model.training
                self.model.eval()
                
                # ä½¿ç”¨torch.no_gradç¡®ä¿æ²¡æœ‰æ¢¯åº¦è®¡ç®—å¹²æ‰°
                with torch.no_grad():
                    # æ­¥éª¤ 1: è·å–DDPæ¨¡å‹çš„çŠ¶æ€å­—å…¸
                    # DDPæ¨¡å¼ä¸‹ç›´æ¥è·å–state_dictï¼Œæ— éœ€ç‰¹æ®ŠAPI
                    full_state_dict = {k: v.clone().detach().cpu() for k, v in self.model.state_dict().items()}

                # æ¢å¤åŸå§‹è®­ç»ƒæ¨¡å¼
                self.model.train(original_training_mode)

                # æ­¥éª¤ 2: ä½¿ç”¨ä¸åˆ›å»ºå‚è€ƒæ¨¡å‹æ—¶å®Œå…¨ç›¸åŒçš„é”®é‡æ˜ å°„é€»è¾‘
                remapped_state_dict = {}
                # DDPå¯èƒ½çš„å‰ç¼€
                possible_prefixes = ["module.model.", "model."]
                
                # æ£€æŸ¥æ”¶åˆ°çš„é”®ï¼Œç¡®å®šä½¿ç”¨å“ªä¸ªå‰ç¼€
                first_key = next(iter(full_state_dict.keys()), "")
                prefix_to_strip = ""
                
                for prefix in possible_prefixes:
                    if any(k.startswith(prefix) for k in full_state_dict.keys()):
                        prefix_to_strip = prefix
                        break
                
                # å¥å£®æ€§æ£€æŸ¥: ç¡®è®¤é¢„æœŸçš„å‰ç¼€å­˜åœ¨
                if not prefix_to_strip:
                     print(f"    âš ï¸ è­¦å‘Š: æ›´æ–°å‚è€ƒæ¨¡å‹æ—¶ï¼Œåœ¨ state_dict ä¸­æœªæ‰¾åˆ°é¢„æœŸçš„å‰ç¼€ã€‚ç¬¬ä¸€ä¸ªé”®æ˜¯ '{first_key}'ã€‚")
                     print(f"    â„¹ï¸ å°†å°è¯•ä¸å‰¥ç¦»å‰ç¼€ç›´æ¥åŠ è½½ã€‚")

                for k, v in full_state_dict.items():
                    if prefix_to_strip and k.startswith(prefix_to_strip):
                        new_key = k[len(prefix_to_strip):]
                        remapped_state_dict[new_key] = v.to(device)  # ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
                    else:
                        remapped_state_dict[k] = v.to(device)
                
                # æ­¥éª¤ 3: åœ¨no_gradç¯å¢ƒä¸‹åŠ è½½æ–°çŠ¶æ€åˆ°å‚è€ƒæ¨¡å‹ä¸­
                with torch.no_grad():
                    self.reference_model.load_state_dict(remapped_state_dict, strict=True)
                print("    âœ… å‚è€ƒæ¨¡å‹æƒé‡æ›´æ–°æˆåŠŸ (DDPæ¨¡å¼)ã€‚")
                
            except Exception as e:
                print(f"    âŒ æ›´æ–°å‚è€ƒæ¨¡å‹æƒé‡å¤±è´¥: {e}")
                print(f"    ğŸ”§ å°è¯•çš„è§£å†³æ–¹æ¡ˆï¼šè·³è¿‡æ­¤æ¬¡æ›´æ–°ï¼Œç»§ç»­è®­ç»ƒ")
                import traceback
                traceback.print_exc()
                return

            # æ­¥éª¤ 4: ç¡®ä¿å‚è€ƒæ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šå¹¶å¤„äºè¯„ä¼°æ¨¡å¼
            self.reference_model = self.reference_model.to(device)
            self.reference_model.eval()

    def _sample_node_counts(self, batch_size: int, device: torch.device) -> torch.Tensor:
        """
        æ ¹æ®é…ç½®é‡‡æ ·èŠ‚ç‚¹æ•°
        
        Args:
            batch_size: æ‰¹é‡å¤§å°
            device: è®¾å¤‡
            
        Returns:
            èŠ‚ç‚¹æ•°å¼ é‡ shape: (batch_size,)
        """
        if self.target_node_count is not None:
            # å›ºå®šèŠ‚ç‚¹æ•°
            return torch.full((batch_size,), self.target_node_count, dtype=torch.long, device=device)
        elif self.node_count_range is not None:
            # èŒƒå›´å†…éšæœºé‡‡æ ·
            min_nodes, max_nodes = self.node_count_range
            return torch.randint(min_nodes, max_nodes + 1, (batch_size,), dtype=torch.long, device=device)
        else:
            raise ValueError("èŠ‚ç‚¹æ•°é…ç½®æ— æ•ˆ")

    def _run_model_forward(self, X_t, E_t, y_t, t, node_mask):
        """
        å¯è¢«æ¢¯åº¦æ£€æŸ¥ç‚¹åŒ…è£…çš„æ¨¡å‹å‰å‘ä¼ æ’­å‡½æ•°ã€‚
        æ³¨æ„: ä¸ºäº†ä¸ `grad_checkpoint` å…¼å®¹, æ­¤å‡½æ•°åªæ¥å—å’Œè¿”å›å¼ é‡ã€‚
        """
        # grad_checkpoint åœ¨ `use_reentrant=False` æ¨¡å¼ä¸‹ä¸æ”¯æŒ None è¾“å…¥,
        # æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå ä½ç¬¦ã€‚ç”±äºæ¨¡å‹å†…éƒ¨ä¼šå¤„ç†æ¡ä»¶/éæ¡ä»¶æƒ…å†µï¼Œ
        # æˆ‘ä»¬å¯ä»¥å®‰å…¨åœ°ä¼ å…¥ä¸€ä¸ªé›¶å¼ é‡ä½œä¸ºå ä½ç¬¦ã€‚
        if y_t is None:
            # åˆ›å»ºä¸€ä¸ªä¸ X_t è®¾å¤‡å’Œç±»å‹ç›¸åŒ¹é…çš„è™šæ‹Ÿå¼ é‡
            y_t = torch.empty(X_t.size(0), 0, device=X_t.device, dtype=X_t.dtype)

        noisy_data = {"X_t": X_t, "E_t": E_t, "y_t": y_t, "t": t, "node_mask": node_mask}
        extra_data = self.core_model.compute_extra_data(noisy_data)
        pred = self.core_model.forward(noisy_data, extra_data, node_mask)
        # grad_checkpoint è¦æ±‚è¾“å‡ºæ˜¯å¼ é‡å…ƒç»„
        return pred.X, pred.E, pred.y

    def _compute_policy_entropy(self, model_pred: utils.PlaceHolder, node_mask: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—ç­–ç•¥çš„ç†µï¼Œç”¨äºé¼“åŠ±æ¢ç´¢ã€‚"""
        # èŠ‚ç‚¹ç†µ
        X_logits = model_pred.X
        X_probs = F.softmax(X_logits, dim=-1)
        X_log_probs = F.log_softmax(X_logits, dim=-1)
        entropy_X = -(X_probs * X_log_probs).sum(dim=-1)
        # ä¸ºé¿å…é™¤ä»¥é›¶ï¼Œæ·»åŠ ä¸€ä¸ªå°çš„epsilon
        masked_entropy_X = (entropy_X * node_mask).sum() / (node_mask.sum() + 1e-8)

        # è¾¹ç†µ
        E_logits = model_pred.E
        edge_mask = node_mask.unsqueeze(1) * node_mask.unsqueeze(2)
        E_probs = F.softmax(E_logits, dim=-1)
        E_log_probs = F.log_softmax(E_logits, dim=-1)
        entropy_E = -(E_probs * E_log_probs).sum(dim=-1)
        # ä¸ºé¿å…é™¤ä»¥é›¶ï¼Œæ·»åŠ ä¸€ä¸ªå°çš„epsilon
        masked_entropy_E = (entropy_E * edge_mask).sum() / (edge_mask.sum() + 1e-8)
        
        return (masked_entropy_X + masked_entropy_E) / 2.0

    def sample_graphs_with_gradients(
        self,
        batch_size: int,
        num_nodes: Optional[torch.Tensor] = None,
        seed: Optional[int] = None,
        total_inference_steps: int = 100,
    ) -> Tuple[utils.PlaceHolder, torch.Tensor, torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:
        """
        ä»å®Œå…¨å™ªå£°çŠ¶æ€å¼€å§‹é‡‡æ ·å›¾ï¼Œä¿æŒæ¢¯åº¦è®¡ç®—å¹¶è®°å½•é‡‡æ ·è½¨è¿¹çš„å¯¹æ•°æ¦‚ç‡ã€‚
        æ­¤ç‰ˆæœ¬å®Œæ•´å¤ç°äº†åŸå§‹æ¨¡å‹çš„é«˜è´¨é‡æ¨ç†é€»è¾‘ï¼Œå¹¶ä¿®å¤äº†å†…å­˜æ³„æ¼é—®é¢˜ã€‚
        
        Args:
            batch_size: æ‰¹é‡å¤§å°
            num_nodes: èŠ‚ç‚¹æ•°å¼ é‡ shape: (batch_size,)ï¼Œå¦‚æœä¸ºNoneåˆ™æ ¹æ®é…ç½®é‡‡æ ·
            seed: éšæœºç§å­
            total_inference_steps: æ€»æ¨ç†æ­¥æ•°
        
        Returns:
            (ç”Ÿæˆçš„å›¾, èŠ‚ç‚¹æ©ç , ç´¯ç§¯æŸå¤±å¼ é‡, å½“å‰ç­–ç•¥å¯¹æ•°æ¦‚ç‡, å‚è€ƒç­–ç•¥å¯¹æ•°æ¦‚ç‡)
        """
        original_mode = self.model.training
        self.model.eval()
        try:
            if seed is not None:
                torch.manual_seed(seed)
                np.random.seed(seed % (2**31))
                random.seed(seed)
            
            device = next(self.model.parameters()).device
            
            # æ­¥éª¤ 1: é‡‡æ ·æˆ–ç¡®å®šèŠ‚ç‚¹æ•°
            if num_nodes is None:
                n_nodes = self._sample_node_counts(batch_size, device)
            elif isinstance(num_nodes, int):
                n_nodes = num_nodes * torch.ones(batch_size, device=device, dtype=torch.int)
            else:
                n_nodes = num_nodes
            
            n_max = torch.max(n_nodes).item()
            
            # æ­¥éª¤ 2: æ„å»ºèŠ‚ç‚¹æ©ç 
            arange = torch.arange(n_max, device=device).unsqueeze(0).expand(batch_size, -1)
            node_mask = arange < n_nodes.unsqueeze(1)
            
            # æ­¥éª¤ 3: ä»å®Œå…¨å™ªå£°çŠ¶æ€ z_T å¼€å§‹
            z_T = flow_matching_utils.sample_discrete_feature_noise(
                limit_dist=self.core_model.noise_dist.get_limit_dist(),
                node_mask=node_mask
            )
            
            # æ­¥éª¤ 4: å¤„ç†æ¡ä»¶æ ‡ç­¾ y (å¦‚æœæ¨¡å‹æ˜¯æ¡ä»¶æ¨¡å‹)
            if self.core_model.conditional:
                # ä¿®æ­£ï¼šä¸åŸå§‹ sample_batch ä¸­çš„é€»è¾‘å®Œå…¨ä¸€è‡´
                if "qm9" in self.cfg.dataset.name:
                    if hasattr(self.core_model, 'test_labels') and self.core_model.test_labels is not None:
                        y = self.core_model.test_labels
                        perm = torch.randperm(y.size(0))
                        idx = perm[:100]
                        condition = y[idx].to(device)
                        z_T.y = condition.repeat([10, 1])[:batch_size, :]
                    else:
                        # å¦‚æœæ²¡æœ‰test_labelsï¼Œç”Ÿæˆéšæœºæ¡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                        z_T.y = torch.randn(batch_size, self.core_model.output_dims["y"]).to(device)
                elif "tls" in self.cfg.dataset.name:
                    z_T.y = torch.zeros(batch_size, 1).to(device)
                    z_T.y[:batch_size // 2] = 1
                else:
                    # ä¿®æ­£ï¼šä¸åŸå§‹å®ç°ä¸€è‡´ï¼Œå¯¹ä¸æ”¯æŒçš„æ•°æ®é›†æŠ›å‡ºå¼‚å¸¸
                    # è¿™ç¡®ä¿æ¡ä»¶å¤„ç†çš„ä¸¥æ ¼ä¸€è‡´æ€§
                    raise NotImplementedError(f"Conditional sampling not implemented for dataset: {self.cfg.dataset.name}")
            
            # æ­¥éª¤ 5: åˆå§‹åŒ–çŠ¶æ€å’Œè®°å½•å˜é‡
            X, E, y = z_T.X, z_T.E, z_T.y
            
            # ğŸ”§ ä¿æŒå®Œæ•´çš„è®¡ç®—å›¾ï¼Œç”¨äºç²¾ç¡®çš„æ¢¯åº¦è®¡ç®—
            cumulative_loss = torch.tensor(0.0, device=device, requires_grad=True)
            current_log_probs = torch.zeros(batch_size, device=device, requires_grad=False)
            reference_log_probs = torch.zeros(batch_size, device=device, requires_grad=False)
            pred_final_step = None
            # --- å‚è€ƒæ¨¡å‹æ›´æ–°æ£€æŸ¥ ---
            # æ£€æŸ¥å½“å‰æ˜¯å¦ä¸ºå‚è€ƒæ¨¡å‹çš„æ›´æ–°æ­¥éª¤ã€‚DDPä¸‹æ­¤æ“ä½œæ›´ç®€å•ï¼Œæ— éœ€ç‰¹æ®Šå¤„ç†ã€‚
            is_ref_update_step = (self.global_step > 0 and self.global_step % self.ref_model_update_freq == 0)
            if is_ref_update_step:
                print(f"   â„¹ï¸ æ­¥éª¤ {self.global_step}: å³å°†æ›´æ–°å‚è€ƒæ¨¡å‹ï¼ˆDDPæ¨¡å¼ï¼‰ã€‚")
            # --- æ£€æŸ¥ç»“æŸ ---

            # æ­¥éª¤ 6: æ ¸å¿ƒæ¨ç†å¾ªç¯
            for t_int in tqdm(range(total_inference_steps), desc="  ...é‡‡æ ·è½¨è¿¹", leave=False):
                # è®¡ç®—å½“å‰æ—¶é—´æ­¥ t å’Œä¸‹ä¸€ä¸ªæ—¶é—´æ­¥ s
                t_array = t_int * torch.ones((batch_size, 1)).type_as(y)
                t_norm = t_array / (total_inference_steps + 1)
                
                if ("absorb" in self.cfg.model.transition) and (t_int == 0):
                    t_norm = t_norm + 1e-6
                
                s_array = t_array + 1
                s_norm = s_array / (total_inference_steps + 1)
                
                # åº”ç”¨æ—¶é—´æ‰­æ›²
                t_norm = self.core_model.time_distorter.sample_ft(
                    t_norm, self.cfg.sample.time_distortion
                )
                s_norm = self.core_model.time_distorter.sample_ft(
                    s_norm, self.cfg.sample.time_distortion
                )
                
                # --- åœ¨æ¯ä¸ªæ­¥éª¤è®¡ç®—ä¸­é—´æŸå¤±å’Œå¯¹æ•°æ¦‚ç‡ ---
                with torch.enable_grad():
                    # å‡†å¤‡å¸¦æ¢¯åº¦çš„æ•°æ®
                    X_temp = X.detach().requires_grad_(True)
                    E_temp = E.detach().requires_grad_(True)
                    if y is not None:
                        if y.dtype.is_floating_point:
                            y_temp = y.detach().requires_grad_(True)
                        else:
                            y_temp = y.detach()
                    else:
                        y_temp = None

                    # æ¿€æ´»æ£€æŸ¥ç‚¹å·²åœ¨GraphTransformerå†…éƒ¨å®ç°ï¼Œæ­¤å¤„ç›´æ¥è°ƒç”¨å³å¯
                    pred_X, pred_E, pred_y = self._run_model_forward(
                        X_temp, E_temp, y_temp, t_norm, node_mask
                    )
                    pred = utils.PlaceHolder(X=pred_X, E=pred_E, y=pred_y)
                    if t_int == total_inference_steps - 1:
                        pred_final_step = pred
                    # è®¡ç®—å¹¶ç´¯ç§¯æŸå¤± (ä½¿ç”¨åŒä¸€æ¬¡å‰å‘ä¼ æ’­çš„ç»“æœ)
                    intermediate_loss = self._compute_intermediate_loss(X_temp, E_temp, pred, node_mask)
                    cumulative_loss = cumulative_loss + intermediate_loss
                    
                    # è®¡ç®—å¹¶ç´¯ç§¯å¯¹æ•°æ¦‚ç‡ (ä½¿ç”¨åŒä¸€æ¬¡å‰å‘ä¼ æ’­çš„ç»“æœ)
                    step_log_prob = self._compute_step_log_probability(X_temp, E_temp, pred, node_mask)
                    current_log_probs = current_log_probs + step_log_prob # ä¿æŒæ¢¯åº¦è¿æ¥
                    
                    # å‚è€ƒç­–ç•¥ (æ— æ¢¯åº¦)
                    with torch.no_grad():
                        self._ensure_reference_model()
                        ref_noisy_data = {"X_t": X, "E_t": E, "y_t": y, "t": t_norm, "node_mask": node_mask}
                        ref_extra_data = self.reference_model.compute_extra_data(ref_noisy_data)
                        ref_pred = self.reference_model.forward(ref_noisy_data, ref_extra_data, node_mask)
                        ref_step_log_prob = self._compute_step_log_probability(X, E, ref_pred, node_mask)
                        reference_log_probs = reference_log_probs + ref_step_log_prob.detach()

                # --- æ ¸å¿ƒé‡‡æ ·é€»è¾‘ï¼šå®Œæ•´å¤ç°åŸå§‹æ¨ç†æµç¨‹ ---
                if t_int < total_inference_steps - 1:
                    # ğŸ”§ åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ä¸ä¿æŒæ¢¯åº¦ï¼Œå› ä¸ºé‡‡æ ·æ“ä½œæœ¬èº«æ˜¯ä¸å¯å¯¼çš„
                    with torch.no_grad():
                        noisy_data_no_grad = {
                            "X_t": X, "E_t": E, "y_t": y, "t": t_norm, "node_mask": node_mask
                        }
                        
                        # 1. è®¡ç®—æœ‰æ¡ä»¶é¢„æµ‹
                        extra_data = self.core_model.compute_extra_data(noisy_data_no_grad)
                        pred = self.core_model.forward(noisy_data_no_grad, extra_data, node_mask)
                        pred_X_cond = F.softmax(pred.X, dim=-1)
                        pred_E_cond = F.softmax(pred.E, dim=-1)
                        
                        # 2. è®¡ç®—æœ‰æ¡ä»¶ rate matrix
                        G_1_pred_cond = (pred_X_cond, pred_E_cond)
                        G_t = (X, E)
                        R_t_X, R_t_E = self.core_model.rate_matrix_designer.compute_graph_rate_matrix(
                            t_norm, node_mask, G_t, G_1_pred_cond,
                        )
                        
                        # 3. å¤ç°æ¡ä»¶å¼•å¯¼ (Conditional Guidance)
                        if self.core_model.conditional:
                            uncond_y = torch.ones_like(y, device=y.device) * -1
                            noisy_data_no_grad["y_t"] = uncond_y
                            
                            extra_data_uncond = self.core_model.compute_extra_data(noisy_data_no_grad)
                            uncond_pred = self.core_model.forward(noisy_data_no_grad, extra_data_uncond, node_mask)
                            uncond_pred_X = F.softmax(uncond_pred.X, dim=-1)
                            uncond_pred_E = F.softmax(uncond_pred.E, dim=-1)
                            
                            G_1_pred_uncond = (uncond_pred_X, uncond_pred_E)
                        
                            R_t_X_uncond, R_t_E_uncond = self.core_model.rate_matrix_designer.compute_graph_rate_matrix(
                                t_norm, node_mask, G_t, G_1_pred_uncond,
                            )
                            
                            # åœ¨å¯¹æ•°ç©ºé—´æ··åˆ rate matrices
                            guidance_weight = self.core_model.cfg.general.guidance_weight
                            R_t_X = torch.exp(
                                torch.log(R_t_X_uncond + 1e-6) * (1 - guidance_weight) +
                                torch.log(R_t_X + 1e-6) * guidance_weight
                            )
                            R_t_E = torch.exp(
                                torch.log(R_t_E_uncond + 1e-6) * (1 - guidance_weight) +
                                torch.log(R_t_E + 1e-6) * guidance_weight
                            )
                        
                        # 4. å¤ç°è½¬ç§»æ¦‚ç‡è®¡ç®— (è°ƒç”¨æ¨¡å‹å†…ç½®å‡½æ•°)
                        dt = (s_norm - t_norm)[0]
                        prob_X, prob_E = self.core_model.compute_step_probs(
                            R_t_X, R_t_E, X, E, dt,
                            self.core_model.noise_dist.get_limit_dist().X,
                            self.core_model.noise_dist.get_limit_dist().E
                        )
                        
                        # 5. å¤ç°æœ€åä¸€æ­¥çš„ç‰¹æ®Šå¤„ç†
                        if s_norm[0] == 1.0:
                            prob_X, prob_E = pred_X_cond, pred_E_cond
                        
                        # 6. ä½¿ç”¨è®¡ç®—å‡ºçš„æ¦‚ç‡è¿›è¡Œé‡‡æ ·
                        sampled_s = flow_matching_utils.sample_discrete_features(
                            prob_X, prob_E, node_mask=node_mask
                        )
                        
                        # 7. æ›´æ–°çŠ¶æ€ä¸º one-hot æ ¼å¼ï¼Œç”¨äºä¸‹ä¸€æ¬¡å¾ªç¯
                        X = F.one_hot(sampled_s.X, num_classes=X.size(-1)).float()
                        E = F.one_hot(sampled_s.E, num_classes=E.size(-1)).float()
                        
                        # ç¡®ä¿è¾¹çŸ©é˜µçš„å¯¹ç§°æ€§
                        assert (E == torch.transpose(E, 1, 2)).all()

            # æ­¥éª¤ 8: è¿”å›æœ€ç»ˆçš„ã€æ¸…ç†è¿‡çš„å›¾

            # ä¿®æ­£ï¼šåœ¨è¿”å›å‰ï¼Œç§»é™¤è™šæ‹Ÿç±»åˆ«ï¼Œä¸åŸå§‹å®ç°(sample_batch)å®Œå…¨ä¸€è‡´
            # è¿™æ˜¯ä¿è¯ç”Ÿæˆå›¾ç»“æ„æ­£ç¡®çš„å…³é”®æ­¥éª¤
            X, E, y = self.core_model.noise_dist.ignore_virtual_classes(X, E, y)

            clean_graphs = utils.PlaceHolder(X=X, E=E, y=y).mask(node_mask, collapse=True)
            
            # ğŸ”§ ä¸»åŠ¨æ¸…ç†å†…å­˜
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
            return clean_graphs, node_mask, cumulative_loss, current_log_probs, reference_log_probs, pred_final_step
        
        finally:
            self.model.train(original_mode)

    def _compute_intermediate_loss(
        self,
        X_t: torch.Tensor,  # shape: (batch_size, n_max, n_node_types)
        E_t: torch.Tensor,  # shape: (batch_size, n_max, n_max, n_edge_types)
        model_pred: utils.PlaceHolder,
        node_mask: torch.Tensor  # shape: (batch_size, n_max)
    ) -> torch.Tensor:
        """
        è®¡ç®—ä¸­é—´çŠ¶æ€çš„æŸå¤±ã€‚
        æ­¤ç‰ˆæœ¬ç»è¿‡é‡æ„ï¼Œç›´æ¥æ¥æ”¶æ¨¡å‹é¢„æµ‹ç»“æœä»¥æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹ã€‚
        """
        # æ¥æ”¶æ¨¡å‹é¢„æµ‹
        pred = model_pred
        
        # è®¡ç®—å½“å‰çŠ¶æ€çš„è´Ÿå¯¹æ•°ä¼¼ç„¶
        X_current = torch.argmax(X_t, dim=-1)  # shape: (batch_size, n_max)
        E_current = torch.argmax(E_t, dim=-1)  # shape: (batch_size, n_max, n_max)
        
        # ä½¿ç”¨äº¤å‰ç†µæŸå¤±
        nll_X = F.cross_entropy(
            pred.X.view(-1, pred.X.size(-1)),
            X_current.view(-1),
            reduction='none'
        ).view(X_current.shape)  # shape: (batch_size, n_max)
        
        nll_E = F.cross_entropy(
            pred.E.view(-1, pred.E.size(-1)),
            E_current.view(-1),
            reduction='none'
        ).view(E_current.shape)  # shape: (batch_size, n_max, n_max)
        
        # åº”ç”¨æ©ç 
        masked_nll_X = (nll_X * node_mask).sum()
        edge_mask = node_mask.unsqueeze(1) * node_mask.unsqueeze(2)  # shape: (batch_size, n_max, n_max)
        masked_nll_E = (nll_E * edge_mask).sum()
        
        return masked_nll_X + masked_nll_E

    def _compute_step_log_probability(
        self,
        X_current: torch.Tensor,  # shape: (batch_size, n_max, n_node_types)
        E_current: torch.Tensor,  # shape: (batch_size, n_max, n_max, n_edge_types)
        model_pred: utils.PlaceHolder,
        node_mask: torch.Tensor,  # shape: (batch_size, n_max)
    ) -> torch.Tensor:
        """
        è®¡ç®—å½“å‰é‡‡æ ·æ­¥éª¤çš„å¯¹æ•°æ¦‚ç‡
        
        Returns:
            è¯¥æ­¥éª¤çš„å¯¹æ•°æ¦‚ç‡ shape: (batch_size,)
        """
        batch_size = X_current.size(0)
        device = X_current.device
        
        # è®¡ç®—èŠ‚ç‚¹è½¬ç§»çš„å¯¹æ•°æ¦‚ç‡
        X_logits = model_pred.X
        X_probs = F.softmax(X_logits, dim=-1)  # shape: (batch_size, n_max, n_node_types)
        X_log_probs = torch.log(X_probs + 1e-8)
        
        # è·å–å®é™…é‡‡æ ·çš„ç±»åˆ«ç´¢å¼•
        X_indices = torch.argmax(X_current, dim=-1)  # shape: (batch_size, n_max)
        
        # æ”¶é›†å¯¹åº”çš„å¯¹æ•°æ¦‚ç‡
        X_step_log_prob = torch.gather(X_log_probs, dim=-1, 
                                      index=X_indices.unsqueeze(-1)).squeeze(-1)
        
        # åº”ç”¨èŠ‚ç‚¹æ©ç å¹¶æ±‚å’Œ
        X_masked_log_prob = (X_step_log_prob * node_mask).sum(dim=-1)  # shape: (batch_size,)
        
        # è®¡ç®—è¾¹è½¬ç§»çš„å¯¹æ•°æ¦‚ç‡
        E_logits = model_pred.E
        E_probs = F.softmax(E_logits, dim=-1)  # shape: (batch_size, n_max, n_max, n_edge_types)
        E_log_probs = torch.log(E_probs + 1e-8)
        
        # è·å–è¾¹çš„ç±»åˆ«ç´¢å¼•
        E_indices = torch.argmax(E_current, dim=-1)  # shape: (batch_size, n_max, n_max)
        
        # æ”¶é›†è¾¹çš„å¯¹æ•°æ¦‚ç‡
        E_step_log_prob = torch.gather(E_log_probs, dim=-1,
                                      index=E_indices.unsqueeze(-1)).squeeze(-1)
        
        # è¾¹æ©ç ï¼šåªè€ƒè™‘æœ‰æ•ˆçš„è¾¹
        edge_mask = node_mask.unsqueeze(1) * node_mask.unsqueeze(2)  # shape: (batch_size, n_max, n_max)
        E_masked_log_prob = (E_step_log_prob * edge_mask).sum(dim=(-2, -1))  # shape: (batch_size,)
        
        # æ€»çš„æ­¥éª¤å¯¹æ•°æ¦‚ç‡
        total_step_log_prob = X_masked_log_prob + E_masked_log_prob
        
        return total_step_log_prob

    def _convert_to_graph_list(self, graphs: utils.PlaceHolder, node_mask: torch.Tensor) -> List:
        """å°†PlaceHolderå›¾è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼"""
        graph_list = []
        X, E, y = graphs.X, graphs.E, graphs.y
        
        for i in range(X.size(0)):
            n_nodes = node_mask[i].sum().item()
            
            # è·å–èŠ‚ç‚¹ç‰¹å¾ï¼šè½¬æ¢ä¸ºç¦»æ•£æ ‡ç­¾
            if X.dim() == 3:  # one-hotç¼–ç  (batch_size, n_nodes, n_node_types)
                atom_types = torch.argmax(X[i, :n_nodes], dim=-1)  # shape: (n_nodes,)
            else:  # å·²ç»æ˜¯ç¦»æ•£æ ‡ç­¾
                atom_types = X[i, :n_nodes]
                
            # è·å–è¾¹ç‰¹å¾ï¼šè½¬æ¢ä¸ºç¦»æ•£æ ‡ç­¾  
            if E.dim() == 4:  # one-hotç¼–ç  (batch_size, n_nodes, n_nodes, n_edge_types)
                edge_types = torch.argmax(E[i, :n_nodes, :n_nodes], dim=-1)  # shape: (n_nodes, n_nodes)
            else:  # å·²ç»æ˜¯ç¦»æ•£æ ‡ç­¾
                edge_types = E[i, :n_nodes, :n_nodes]
            
            # å¤„ç†ç‰¹æ®Šæƒ…å†µï¼šå½“åªæœ‰ä¸€ä¸ªèŠ‚ç‚¹æ—¶
            if n_nodes == 1:
                # atom_typeså·²ç»æ˜¯(1,)å½¢çŠ¶ï¼Œæ— éœ€è°ƒæ•´
                # edge_typeså·²ç»æ˜¯(1,1)å½¢çŠ¶ï¼Œæ— éœ€è°ƒæ•´
                pass
            
            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            if atom_types.dtype != torch.long:
                atom_types = atom_types.long()
            if edge_types.dtype != torch.long:
                edge_types = edge_types.long()
                
            graph_list.append([atom_types, edge_types])
        
        return graph_list

    def compute_grpo_loss(
        self,
        rewards: torch.Tensor,
        current_log_probs: torch.Tensor,
        reference_log_probs: torch.Tensor,
        model_preds: utils.PlaceHolder,
        node_masks: torch.Tensor,
        global_rank: int = 0,
    ) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—GRPOæŸå¤±ã€KLæ•£åº¦ä»¥åŠç­–ç•¥ç†µå¥–åŠ± (å·²é€‚é…DDPæ¢¯åº¦æµ)ã€‚
        """
        import torch.distributed as dist
        device = rewards.device
        loss_metrics = {}

        # --- å®‰å…¨æ£€æŸ¥ ---
        if rewards.numel() == 0:
            if global_rank == 0:
                print("âš ï¸ [Rank 0] GRPOæŸå¤±è®¡ç®—: è¾“å…¥çš„rewardså¼ é‡ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
            return {"total_loss": torch.tensor(0.0, device=device, requires_grad=True)}

        # --- å…¨å±€åŸºçº¿å’ŒæŒ‡æ ‡è®¡ç®— ---
        # ç”±äºæ•°æ®å·²ç»å…¨å±€åŒæ­¥ï¼Œç›´æ¥è®¡ç®—å³å¯
        global_baseline_reward = rewards.mean()
        loss_metrics['reward/baseline_reward_global'] = global_baseline_reward.item()
        loss_metrics['reward/max_reward_global'] = rewards.max().item()
        loss_metrics['reward/min_reward_global'] = rewards.min().item()

        # --- ç­–ç•¥æŸå¤±è®¡ç®— ---
        # è£å‰ª log_ratio é˜²æ­¢ exp() æº¢å‡º
        log_ratio = current_log_probs - reference_log_probs.detach()
        log_ratio = torch.clamp(log_ratio, min=-15.0, max=15.0)
        
        ratio = torch.exp(log_ratio)
        advantage = rewards - global_baseline_reward
        
        loss_unclipped = ratio * advantage
        loss_clipped = torch.clamp(ratio, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * advantage
        
        # ç›´æ¥è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„å¹³å‡æŸå¤±
        policy_loss = -torch.min(loss_unclipped, loss_clipped).mean()
        
        # --- KL æ•£åº¦æƒ©ç½šè®¡ç®— ---
        kl_divergence = log_ratio.mean()
            
        # --- ç­–ç•¥ç†µè®¡ç®— ---
        policy_entropy = self._compute_policy_entropy(model_preds, node_masks)

        # --- æ€»æŸå¤± ---
        # è£å‰ª KL æ•£åº¦é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        kl_divergence_for_loss = torch.clamp(kl_divergence, min=-10.0, max=10.0) 
        ent_coef = getattr(self.cfg.grpo, 'ent_coef', 0.01)
        total_loss = policy_loss + self.beta * kl_divergence_for_loss - ent_coef * policy_entropy
        
        if global_rank == 0:
            print(f"ğŸ“Š [Rank 0] GRPOæŸå¤±: æ€»è®¡={total_loss.item():.4f}, ç­–ç•¥={policy_loss.item():.4f}, KL={kl_divergence.item():.4f}, ç†µ={policy_entropy.item():.4f}")

        loss_metrics.update({
            'loss/total_loss': total_loss.item(),
            'loss/policy_loss': policy_loss.item(),
            'loss/kl_divergence': kl_divergence.item(), # è®°å½•åŸå§‹KL
            'loss/policy_entropy': policy_entropy.item(),
            'grpo_stats/avg_importance_ratio': ratio.mean().item(),
            'grpo_stats/avg_advantage': advantage.mean().item(),
        })

        return {"total_loss": total_loss, "metrics": loss_metrics}
    

    def sample_and_compute_single_group(self, global_rank: int = 0) -> Tuple[List, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, Dict]:
        """
        é‡‡æ ·å¹¶è®¡ç®—å•ä¸ªgroupï¼Œå¹¶è¿”å›è¯¦ç»†çš„æŒ‡æ ‡å­—å…¸ç”¨äºæ—¥å¿—è®°å½•ã€‚
        
        Returns:
            (ç»„æ•°æ®, å¥–åŠ±å¼ é‡, ç´¯ç§¯æŸå¤±å¼ é‡, å½“å‰å¯¹æ•°æ¦‚ç‡, å‚è€ƒå¯¹æ•°æ¦‚ç‡, æŒ‡æ ‡å­—å…¸)
        """
        start_time = time.time()
        
        # æ‰“å°ä¼˜åŒ–: ä¿ç•™å…³é”®çš„èµ·å§‹ä¿¡æ¯
        unique_seed = int(time.time() * 1000) % (2**32) + global_rank
        print(f"ğŸ¯ [Rank {global_rank}] é‡‡æ · group_size={self.group_size}, seed={unique_seed}")
        
        graphs, node_mask, cumulative_loss, current_log_prob, reference_log_prob, model_pred = self.sample_graphs_with_gradients(
            batch_size=self.group_size,
            seed=unique_seed
        )
        
        sampling_duration = time.time() - start_time
        
        graph_list = self._convert_to_graph_list(graphs, node_mask)
        
        reward_start_time = time.time()
        rewards = self.reward_function(graph_list)
        rewards = rewards.to(next(self.model.parameters()).device)
        reward_duration = time.time() - reward_start_time
        
        duration = time.time() - start_time
        
        # æ‰“å°ä¼˜åŒ–: åˆå¹¶ä¸ºä¸€ä¸ªæ¸…æ™°çš„å®Œæˆæ‘˜è¦
        avg_reward = rewards.mean().item()
        loss_val = cumulative_loss.mean().item()
        print(f"âœ… [Rank {global_rank}] Groupå®Œæˆ (è€—æ—¶ {duration:.2f}s): å¹³å‡å¥–åŠ±={avg_reward:.4f}, ç´¯ç§¯æŸå¤±={loss_val:.2f}")

        # SwanLabæ—¥å¿—: åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰è¯¦ç»†æŒ‡æ ‡çš„å­—å…¸
        group_metrics = {
            f"perf/group_duration_sec_rank_{global_rank}": duration,
            f"perf/sampling_duration_sec_rank_{global_rank}": sampling_duration,
            f"perf/reward_duration_sec_rank_{global_rank}": reward_duration,
            f"reward/avg_reward_rank_{global_rank}": avg_reward,
            f"loss/cumulative_loss_rank_{global_rank}": loss_val,
            f"probs/current_log_prob_rank_{global_rank}": current_log_prob.mean().item(),
            f"probs/reference_log_prob_rank_{global_rank}": reference_log_prob.mean().item(),
        }
        
        return [graph_list], rewards, cumulative_loss, current_log_prob, reference_log_prob, model_pred, node_mask, group_metrics


    def state_dict(self) -> Dict:
        """è¿”å›åŒ…å«è®­ç»ƒå™¨çŠ¶æ€çš„å­—å…¸ï¼Œç”¨äºä¿å­˜æ£€æŸ¥ç‚¹"""
        state = {
            'global_step': self.global_step,
        }
        # ä»…å½“ original_model_state åˆå§‹åŒ–åæ‰ä¿å­˜
        if self.original_model_state is not None:
            state['original_model_state'] = self.original_model_state
        return state

    def load_state_dict(self, state_dict: Dict):
        """ä»çŠ¶æ€å­—å…¸ä¸­åŠ è½½è®­ç»ƒå™¨çŠ¶æ€"""
        # self.optimizer.load_state_dict(state_dict['optimizer_state_dict'])
        self.global_step = state_dict['global_step']
        # å…¼å®¹å¤„ç†å¯èƒ½ä¸å­˜åœ¨çš„ key
        if 'original_model_state' in state_dict:
            self.original_model_state = state_dict['original_model_state']
        print(f"âœ… GRPOTrainer çŠ¶æ€å·²ä»æ£€æŸ¥ç‚¹åŠ è½½ (global_step: {self.global_step})")

    def save_checkpoint(self, filepath: str):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        torch.save(self.state_dict(), filepath)
        print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {filepath}")

    def load_checkpoint(self, filepath: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        state_dict = torch.load(filepath, map_location=next(self.model.parameters()).device)
        self.load_state_dict(state_dict)
        # å…¼å®¹æ—§ç‰ˆæ£€æŸ¥ç‚¹
        if 'model_state_dict' in state_dict:
            self.model.load_state_dict(state_dict['model_state_dict'])
        print(f"æ£€æŸ¥ç‚¹å·²ä» {filepath} åŠ è½½")