"""
ä½¿ç”¨PyTorch Lightningæ¡†æ¶çš„GRPOè®­ç»ƒä¸»æ–‡ä»¶
ä¸åŸå§‹è®­ç»ƒçš„main.pyä¿æŒç›¸åŒçš„æ¨¡å¼ï¼Œä»…æ·»åŠ GRPOç‰¹æœ‰çš„é€»è¾‘
"""

import os
import time
import torch
import hydra
import numpy as np
import random
import warnings
from omegaconf import DictConfig
import pytorch_lightning as pl
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor
from pytorch_lightning.strategies import DDPStrategy
from pytorch_lightning.utilities.warnings import PossibleUserWarning
from omegaconf import OmegaConf

try:
    import swanlab
except ImportError:
    swanlab = None

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å‹å’Œæ¨¡å—
from graph_discrete_flow_model import GraphDiscreteFlowModel
from grpo_lightning_module import (
    GRPOLightningModule, 
    DummyDataModule,
    create_grpo_lightning_module,
    run_grpo_lightning_testing
)

# å¯¼å…¥æˆ‘ä»¬çš„Lightningæ¨¡å—
from grpo_lightning_module import (
    GRPOLightningModule, 
    DummyDataModule,
    run_grpo_lightning_testing
)

# è®¾ç½®è­¦å‘Šè¿‡æ»¤
warnings.filterwarnings("ignore", category=PossibleUserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning, module="torch_geometric")

# è®¾ç½®CUDAä¼˜åŒ–
torch.set_float32_matmul_precision('medium')  # å¤„ç†Tensor Coresè­¦å‘Š


def create_datamodule_and_components(cfg: DictConfig):
    """
    åˆ›å»ºæ•°æ®æ¨¡å—å’Œæ¨¡å‹ç»„ä»¶ï¼Œä¸åŸå§‹main.pyä¿æŒå®Œå…¨ä¸€è‡´çš„é€»è¾‘
    
    Args:
        cfg: é…ç½®å¯¹è±¡
        
    Returns:
        tuple: (datamodule, model_kwargs)
    """
    dataset_config = cfg["dataset"]
    
    if dataset_config["name"] in [
        "sbm",
        "comm20", 
        "planar",
        "tree",
    ]:
        from analysis.visualization import NonMolecularVisualization
        from datasets.spectre_dataset import (
            SpectreGraphDataModule,
            SpectreDatasetInfos,
        )
        from analysis.spectre_utils import (
            PlanarSamplingMetrics,
            SBMSamplingMetrics,
            Comm20SamplingMetrics,
            TreeSamplingMetrics,
        )
        from metrics.abstract_metrics import TrainAbstractMetricsDiscrete
        from models.extra_features import DummyExtraFeatures, ExtraFeatures

        datamodule = SpectreGraphDataModule(cfg)
        if dataset_config["name"] == "sbm":
            sampling_metrics = SBMSamplingMetrics(datamodule)
        elif dataset_config["name"] == "comm20":
            sampling_metrics = Comm20SamplingMetrics(datamodule)
        elif dataset_config["name"] == "planar":
            sampling_metrics = PlanarSamplingMetrics(datamodule)
        elif dataset_config["name"] == "tree":
            sampling_metrics = TreeSamplingMetrics(datamodule)
        else:
            raise NotImplementedError(
                f"Dataset {dataset_config['name']} not implemented"
            )

        dataset_infos = SpectreDatasetInfos(datamodule, dataset_config)
        train_metrics = TrainAbstractMetricsDiscrete()
        visualization_tools = NonMolecularVisualization(dataset_name=cfg.dataset.name)

        extra_features = ExtraFeatures(
            cfg.model.extra_features,
            cfg.model.rrwp_steps,
            dataset_info=dataset_infos,
        )
        domain_features = DummyExtraFeatures()

        dataset_infos.compute_input_output_dims(
            datamodule=datamodule,
            extra_features=extra_features,
            domain_features=domain_features,
        )
        
    elif dataset_config["name"] in ["my_tree", "my_planar"]:
        from analysis.visualization import NonMolecularVisualization
        from metrics.abstract_metrics import TrainAbstractMetricsDiscrete
        from models.extra_features import DummyExtraFeatures, ExtraFeatures
        
        if dataset_config["name"] == "my_tree":
            from datasets.my_tree_dataset import MyTreeGraphDataModule, MyTreeDatasetInfos
            from analysis.spectre_utils import TreeSamplingMetrics
            
            datamodule = MyTreeGraphDataModule(cfg)
            dataset_infos = MyTreeDatasetInfos(datamodule, dataset_config)
            sampling_metrics = TreeSamplingMetrics(datamodule)
        else:  # my_planar
            from datasets.my_planar_dataset import MyPlanarGraphDataModule, MyPlanarDatasetInfos
            from analysis.spectre_utils import PlanarSamplingMetrics
            
            datamodule = MyPlanarGraphDataModule(cfg)
            dataset_infos = MyPlanarDatasetInfos(datamodule, dataset_config)
            sampling_metrics = PlanarSamplingMetrics(datamodule)

        train_metrics = TrainAbstractMetricsDiscrete()
        visualization_tools = NonMolecularVisualization(dataset_name=cfg.dataset.name)

        extra_features = ExtraFeatures(
            cfg.model.extra_features,
            cfg.model.rrwp_steps,
            dataset_info=dataset_infos,
        )
        domain_features = DummyExtraFeatures()

        dataset_infos.compute_input_output_dims(
            datamodule=datamodule,
            extra_features=extra_features,
            domain_features=domain_features,
        )
        
    elif dataset_config["name"] in ["qm9", "guacamol", "moses"]:
        from metrics.molecular_metrics import (
            TrainMolecularMetrics,
            SamplingMolecularMetrics,
        )
        from metrics.molecular_metrics_discrete import TrainMolecularMetricsDiscrete
        from models.extra_features_molecular import ExtraMolecularFeatures
        from analysis.visualization import MolecularVisualization
        from models.extra_features import ExtraFeatures

        if "qm9" in dataset_config["name"]:
            from datasets import qm9_dataset

            datamodule = qm9_dataset.QM9DataModule(cfg)
            dataset_infos = qm9_dataset.QM9infos(datamodule=datamodule, cfg=cfg)
            dataset_smiles = qm9_dataset.get_smiles(
                cfg=cfg,
                datamodule=datamodule,
                dataset_infos=dataset_infos,
                evaluate_datasets=False,
            )
        elif dataset_config["name"] == "guacamol":
            from datasets import guacamol_dataset

            datamodule = guacamol_dataset.GuacamolDataModule(cfg)
            dataset_infos = guacamol_dataset.Guacamolinfos(datamodule, cfg)
            dataset_smiles = guacamol_dataset.get_smiles(
                raw_dir=datamodule.train_dataset.raw_dir,
                filter_dataset=cfg.dataset.filter,
            )
        elif dataset_config.name == "moses":
            from datasets import moses_dataset

            datamodule = moses_dataset.MosesDataModule(cfg)
            dataset_infos = moses_dataset.MOSESinfos(datamodule, cfg)
            dataset_smiles = moses_dataset.get_smiles(
                raw_dir=datamodule.train_dataset.raw_dir,
                filter_dataset=cfg.dataset.filter,
            )
        else:
            raise ValueError("Dataset not implemented")

        extra_features = ExtraFeatures(
            cfg.model.extra_features,
            cfg.model.rrwp_steps,
            dataset_info=dataset_infos,
        )
        domain_features = ExtraMolecularFeatures(dataset_infos=dataset_infos)

        dataset_infos.compute_input_output_dims(
            datamodule=datamodule,
            extra_features=extra_features,
            domain_features=domain_features,
        )

        train_metrics = TrainMolecularMetricsDiscrete(dataset_infos)

        # We do not evaluate novelty during training
        add_virtual_states = "absorbing" == cfg.model.transition
        sampling_metrics = SamplingMolecularMetrics(
            dataset_infos, dataset_smiles, cfg, add_virtual_states=add_virtual_states
        )
        visualization_tools = MolecularVisualization(
            cfg.dataset.remove_h, dataset_infos=dataset_infos
        )
        
    else:
        raise NotImplementedError("Unknown dataset {}".format(cfg["dataset"]))

    dataset_infos.compute_reference_metrics(
        datamodule=datamodule,
        sampling_metrics=sampling_metrics,
    )

    model_kwargs = {
        "dataset_infos": dataset_infos,
        "train_metrics": train_metrics,
        "sampling_metrics": sampling_metrics,
        "visualization_tools": visualization_tools,
        "extra_features": extra_features,
        "domain_features": domain_features,
        "test_labels": (
            datamodule.test_labels
            if ("qm9" in cfg.dataset.name and cfg.general.conditional)
            else None
        ),
    }
    
    return datamodule, model_kwargs


def create_grpo_callbacks(cfg: DictConfig):
    """åˆ›å»ºGRPOè®­ç»ƒçš„å›è°ƒå‡½æ•°"""
    callbacks = []
    
    # Checkpointå›è°ƒ - ä¸åŸå§‹main.pyä¿æŒä¸€è‡´çš„æ–¹å¼
    if getattr(cfg.grpo, 'save_model', True):
        checkpoint_callback = ModelCheckpoint(
            dirpath=f"checkpoints/grpo_{cfg.general.name}",
            filename="grpo-{epoch:02d}-{step}",
            save_top_k=-1,  # ä¿å­˜æ‰€æœ‰checkpoint
            every_n_train_steps=cfg.grpo.save_every,  # æŒ‰æ­¥æ•°ä¿å­˜
            save_on_train_epoch_end=False,  # ä¸åœ¨epochç»“æŸæ—¶ä¿å­˜
            auto_insert_metric_name=False,
        )
        callbacks.append(checkpoint_callback)
    
    # å­¦ä¹ ç‡ç›‘æ§ - åªæœ‰åœ¨æœ‰loggeræ—¶æ‰æ·»åŠ 
    # lr_monitor = LearningRateMonitor(logging_interval='step')
    # callbacks.append(lr_monitor)
    
    return callbacks


def run_grpo_lightning_training(cfg: DictConfig):
    global_rank = int(os.environ.get("RANK", "0"))
    
    if global_rank == 0:
        print("ğŸš€ å¼€å§‹PyTorch Lightning GRPOè®­ç»ƒ (ä¸»è¿›ç¨‹ Rank 0)")
        print(f"æ•°æ®é›†: {cfg.dataset.name}")
        print(f"å¥–åŠ±å‡½æ•°: {cfg.grpo.reward_type}")
        print(f"é¢„è®­ç»ƒæ¨¡å‹: {cfg.grpo.pretrained_checkpoint}")
    
    # ğŸ¦¢ åˆå§‹åŒ– SwanLab (ä»…åœ¨ä¸»è¿›ç¨‹ä¸­è¿›è¡Œ)
    if global_rank == 0 and swanlab is not None:
        print("ğŸ¦¢ [Rank 0] æ­£åœ¨åˆå§‹åŒ– SwanLab...")
        try:
            config_dict = OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True)
            swanlab.init(
                project=f"GRPO-{cfg.dataset.name}",
                experiment_name=cfg.general.name,
                config=config_dict,
                logdir="./swanlab_logs"
            )
            print("âœ… [Rank 0] SwanLab åˆå§‹åŒ–æˆåŠŸï¼")
        except Exception as e:
            print(f"âš ï¸ [Rank 0] SwanLab åˆå§‹åŒ–å¤±è´¥: {e}")
    
    # è®¾ç½®éšæœºç§å­ - æ‰€æœ‰è¿›ç¨‹éƒ½éœ€è¦è®¾ç½®ä»¥ä¿è¯æ•°æ®åŠ è½½ç­‰éƒ¨åˆ†çš„åŒæ­¥
    pl.seed_everything(cfg.train.seed)
    if global_rank == 0:
        print(f"ğŸ² [Rank 0] ä½¿ç”¨éšæœºç§å­: {cfg.train.seed}")

    # GPUè®¾ç½®
    use_gpu = cfg.general.gpus > 0 and torch.cuda.is_available()
    if use_gpu:
        available_gpus = min(cfg.general.gpus, torch.cuda.device_count())
        print(f"ğŸ–¥ï¸  å¯ç”¨GPUæ•°é‡: {available_gpus}")
        for i in range(available_gpus):
            print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
    else:
        print("ğŸ–¥ï¸  ä½¿ç”¨CPU")
        available_gpus = 0
    
    # 1. åˆ›å»ºæ•°æ®æ¨¡å—å’Œæ¨¡å‹ç»„ä»¶
    try:
        datamodule, model_kwargs = create_datamodule_and_components(cfg)
        print("âœ… æ•°æ®æ¨¡å—å’Œæ¨¡å‹ç»„ä»¶åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ•°æ®æ¨¡å—å’Œæ¨¡å‹ç»„ä»¶åˆ›å»ºå¤±è´¥: {e}")
        raise e
    
    # 2. æ£€æŸ¥é¢„è®­ç»ƒcheckpoint
    if not cfg.grpo.pretrained_checkpoint:
        raise ValueError("âŒ é”™è¯¯: GRPOè®­ç»ƒå¿…é¡»æŒ‡å®š pretrained_checkpoint!")
    
    # 3. åˆ›å»ºGRPO Lightningæ¨¡å—
    try:
        # device = torch.device(f"cuda:0" if use_gpu else "cpu") # Trainerä¼šè‡ªåŠ¨å¤„ç†è®¾å¤‡
        
        # å°† trainer çš„ total_steps ä¼ é€’ä¸‹å»
        grpo_module = create_grpo_lightning_module(
            cfg=cfg,
            model_kwargs=model_kwargs,
            datamodule=datamodule,
            total_steps=cfg.grpo.total_steps,
        )
        print("âœ… GRPO Lightningæ¨¡å—åˆ›å»ºæˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ GRPO Lightningæ¨¡å—åˆ›å»ºå¤±è´¥: {e}")
        raise e
    
    # 4. åˆ›å»ºè™šæ‹Ÿæ•°æ®æ¨¡å—ï¼ˆLightningæ¡†æ¶éœ€è¦ï¼‰
    # é‡è¦ï¼šnum_groupså¿…é¡»ç­‰äºGPUæ•°é‡ï¼Œè¿™æ ·DDPä¼šç»™æ¯å¼ å¡åˆ†é…ä¸€ä¸ªgroup
    num_groups = cfg.grpo.num_groups
    if use_gpu and available_gpus != num_groups:
        print(f"âš ï¸ è­¦å‘Š: GPUæ•°é‡({available_gpus})ä¸num_groups({num_groups})ä¸åŒ¹é…")
        print(f"ğŸ”§ è‡ªåŠ¨è°ƒæ•´: å°†num_groupsè®¾ç½®ä¸º{available_gpus}ä»¥åŒ¹é…GPUæ•°é‡")
        num_groups = available_gpus
    
    dummy_datamodule = DummyDataModule(num_groups=num_groups, num_workers=0)
    
    # 5. åˆ›å»ºå›è°ƒå‡½æ•°
    callbacks = create_grpo_callbacks(cfg)
    
    # 6. åˆ›å»ºLightning Trainer - ä¸åŸå§‹main.pyä¿æŒä¸€è‡´çš„é…ç½®é£æ ¼
    trainer_kwargs = {
        # åŸºç¡€é…ç½®
        'max_steps': cfg.grpo.total_steps,  # ä½¿ç”¨æ­¥æ•°è€Œä¸æ˜¯epoch
        'max_epochs': -1,  # æ— é™åˆ¶epochï¼Œç”±max_stepsæ§åˆ¶
        
        # æ¢¯åº¦å’Œä¼˜åŒ– - ä¸åŸå§‹main.pyç›¸ä¼¼
        'gradient_clip_val': getattr(cfg.grpo, 'gradient_clip_val', 1.0),
        'gradient_clip_algorithm': "norm", # DDPæ”¯æŒnormç®—æ³•ï¼Œæ•ˆæœé€šå¸¸æ›´å¥½
        'accumulate_grad_batches': getattr(cfg.grpo, 'gradient_accumulation_steps', 1),
        
        # éªŒè¯å’Œä¿å­˜
        'check_val_every_n_epoch': None,  # ç¦ç”¨éªŒè¯
        'val_check_interval': None,
        'num_sanity_val_steps': 0,  # è·³è¿‡éªŒè¯sanity check
        
        # æ—¥å¿—å’Œè¿›åº¦
        'log_every_n_steps': getattr(cfg.grpo, 'log_every_n_steps', 50),
        'enable_progress_bar': True,
        'enable_model_summary': True,
        
        # å›è°ƒ
        'callbacks': callbacks,
        
        # è°ƒè¯•
        'fast_dev_run': cfg.general.name == "debug",
        
        # å…¶ä»–
        'deterministic': False,  # GRPOéœ€è¦éšæœºæ€§
        'benchmark': True,  # ä¼˜åŒ–CUDAæ€§èƒ½
        'logger': [],  # æš‚æ—¶ç¦ç”¨logger
    }
    
    # GPUé…ç½® - ä½¿ç”¨DDPç­–ç•¥å®ç°"å…ˆæ²»æœ¬ï¼Œåæ‰©å®¹"æ–¹æ³•è®º
    if use_gpu:
        # ç¬¬äºŒé˜¶æ®µï¼šæ‰©å®¹ - ä½¿ç”¨æ ‡å‡†DDPç­–ç•¥å°†å•å¡ä¼˜åŒ–æˆæœçº¿æ€§æ‰©å±•åˆ°å¤šå¡
        trainer_kwargs.update({
            'accelerator': "gpu",
            'devices': available_gpus,
            'strategy': DDPStrategy(static_graph=True),  # æ˜ç¡®ä½¿ç”¨DDPç­–ç•¥å¹¶å£°æ˜é™æ€å›¾
            'precision': '32'
        })
        print(f"âœ… å·²é…ç½®DDPç­–ç•¥ï¼ˆé™æ€å›¾æ¨¡å¼ï¼‰ï¼šå°†åœ¨ {available_gpus} å¼ GPUä¸Šè¿›è¡Œå¹¶è¡Œè®­ç»ƒ")
    else:
        # CPUé…ç½®
        trainer_kwargs.update({
            'accelerator': "cpu",
            'devices': 1,
        })
    
    trainer = Trainer(**trainer_kwargs)
    print("âœ… Lightning Traineråˆ›å»ºå®Œæˆ")
    

    
    # ------------------- æ ¸å¿ƒè®­ç»ƒé€»è¾‘ (é‡æ„å) -------------------
    try:
        print("ğŸš€ å¼€å§‹GRPO Lightningè®­ç»ƒ...")
        print(f"   æ€»æ­¥æ•°: {cfg.grpo.total_steps}")
        print(f"   å­¦ä¹ ç‡: {cfg.grpo.learning_rate}")
        
        # æ£€æŸ¥æ˜¯æ¢å¤è®­ç»ƒè¿˜æ˜¯ä»å¤´å¼€å§‹å¾®è°ƒ
        ckpt_path_to_resume = cfg.grpo.get('resume_from_checkpoint')

        if ckpt_path_to_resume and os.path.exists(ckpt_path_to_resume):
            # --- åœºæ™¯ B: æ¢å¤ä¸­æ–­çš„GRPOè®­ç»ƒ ---
            print(f"ğŸ“¥ ä»GRPO checkpointæ¢å¤å®Œæ•´è®­ç»ƒçŠ¶æ€: {ckpt_path_to_resume}")
            # ç›´æ¥å°†ckpt_pathä¼ é€’ç»™fit()ï¼ŒLightningä¼šå¤„ç†ä¸€åˆ‡
            trainer.fit(
                model=grpo_module,
                datamodule=dummy_datamodule,
                ckpt_path=ckpt_path_to_resume,
            )
        else:
            # ğŸ”§ DDPåˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–
            # DDPä½¿ç”¨æ ‡å‡†çš„åˆ†å¸ƒå¼åˆå§‹åŒ–ï¼ŒLightningä¼šè‡ªåŠ¨å¤„ç†å¤§éƒ¨åˆ†æƒ…å†µ
            if not torch.distributed.is_initialized():
                if "WORLD_SIZE" in os.environ:
                    # ç”± torchrun/slurm ç­‰å¯åŠ¨å™¨è®¾ç½®çš„å¤šå¡ç¯å¢ƒ
                    rank = int(os.environ["RANK"])
                    world_size = int(os.environ["WORLD_SIZE"])
                    print(f"ğŸŒ æ£€æµ‹åˆ°å¯åŠ¨å™¨ç¯å¢ƒ: Rank {rank}/{world_size}. åˆå§‹åŒ–åˆ†å¸ƒå¼ç»„...")
                    # DDPé€šå¸¸ä½¿ç”¨'nccl'åç«¯è¿›è¡ŒGPUé€šä¿¡
                    torch.distributed.init_process_group(backend="nccl")
                    print("âœ… DDPåˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ (ä½¿ç”¨å¯åŠ¨å™¨å˜é‡)ã€‚")
                else:
                    # å•å¡è¿è¡Œï¼Œæ‰‹åŠ¨è®¾ç½®è™šæ‹Ÿç¯å¢ƒ
                    print("ğŸ”§ æœªæ£€æµ‹åˆ°å¯åŠ¨å™¨ç¯å¢ƒï¼Œä¸ºå•å¡è®­ç»ƒåˆå§‹åŒ–å•è¿›ç¨‹ç»„...")
                    os.environ["MASTER_ADDR"] = "localhost"
                    os.environ["MASTER_PORT"] = "12355"
                    os.environ["RANK"] = "0"
                    os.environ["WORLD_SIZE"] = "1"
                    torch.distributed.init_process_group(backend="gloo", rank=0, world_size=1)
                    print("âœ… å•è¿›ç¨‹ç»„åˆå§‹åŒ–æˆåŠŸã€‚")

            # --- åœºæ™¯ A: ä»é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹æ–°çš„å¾®è°ƒ ---
            pretrained_path = cfg.grpo.pretrained_checkpoint
            if pretrained_path and os.path.exists(pretrained_path):
                print(f"ğŸ“¥ ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æƒé‡ (å¼€å§‹æ–°çš„GRPOè®­ç»ƒ): {pretrained_path}")
                
                # æ‰‹åŠ¨åŠ è½½æƒé‡ï¼Œä½†ä¸åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
                checkpoint = torch.load(pretrained_path, map_location='cpu')

                # --- é‡æ˜ å°„ state_dict çš„é”® ---
                # é¢„è®­ç»ƒæ¨¡å‹æ˜¯ GraphDiscreteFlowModelï¼Œè€Œæˆ‘ä»¬è¦åŠ è½½åˆ° GRPOLightningModule ä¸­ï¼Œ
                # GRPOLightningModule å°† GraphDiscreteFlowModel å­˜åœ¨ self.model å±æ€§ä¸‹ã€‚
                # å› æ­¤ï¼Œé¢„è®­ç»ƒçš„é”® (e.g., 'model.layers.0.weight') éœ€è¦è¢«æ˜ å°„ä¸º
                # 'model.model.layers.0.weight' æ‰èƒ½æ­£ç¡®åŠ è½½åˆ°DDPåŒ…è£…çš„æ¨¡å‹ä¸­ã€‚
                remapped_state_dict = {}
                for k, v in checkpoint['state_dict'].items():
                    # ä¸ºæ‰€æœ‰æ¥è‡ªåŸå§‹ state_dict çš„é”®æ·»åŠ  'model.' å‰ç¼€
                    new_key = f"model.{k}"
                    remapped_state_dict[new_key] = v
                print("âœ… é”®åé‡æ˜ å°„å®Œæˆã€‚")
                
                grpo_module.load_state_dict(remapped_state_dict, strict=False)
                print("âœ… é¢„è®­ç»ƒæƒé‡å·²åŠ è½½åˆ°æ¨¡å‹ã€‚")

          
            trainer.fit(
                model=grpo_module,
                datamodule=dummy_datamodule
            )
        
        print("âœ… GRPO Lightningè®­ç»ƒå®Œæˆ!")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_checkpoint_path = f"/home/ly/max/checkpoints/grpo_{cfg.general.name}/final_model.ckpt"
        trainer.save_checkpoint(final_checkpoint_path)
        print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹ä¿å­˜è‡³: {final_checkpoint_path}")
        
    except Exception as e:
        print(f"âŒ GRPO Lightningè®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise e
    
    finally:
        # æ¸…ç†èµ„æº
        if use_gpu:
            torch.cuda.empty_cache()
    
    return grpo_module


def run_grpo_lightning_sampling(cfg: DictConfig, checkpoint_path: str):
    """
    ä½¿ç”¨GRPOæ¨¡å—è¿›è¡Œçº¯é‡‡æ ·å’Œè¯„ä¼°ï¼Œä½¿ç”¨å¥–åŠ±å‡½æ•°ä½œä¸ºæŒ‡æ ‡
    """
    print(f"ğŸš€ å¼€å§‹ GRPO Lightning é‡‡æ ·æ¨¡å¼: {checkpoint_path}")
    pl.seed_everything(cfg.train.seed)
    if not torch.distributed.is_initialized():
        if "WORLD_SIZE" in os.environ:
            # ç”± torchrun/slurm ç­‰å¯åŠ¨å™¨è®¾ç½®çš„å¤šå¡ç¯å¢ƒ
            rank = int(os.environ["RANK"])
            world_size = int(os.environ["WORLD_SIZE"])
            print(f"ğŸŒ æ£€æµ‹åˆ°å¯åŠ¨å™¨ç¯å¢ƒ: Rank {rank}/{world_size}. åˆå§‹åŒ–åˆ†å¸ƒå¼ç»„...")
            # DDPé€šå¸¸ä½¿ç”¨'nccl'åç«¯è¿›è¡ŒGPUé€šä¿¡
            torch.distributed.init_process_group(backend="nccl")
            print("âœ… DDPåˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ (ä½¿ç”¨å¯åŠ¨å™¨å˜é‡)ã€‚")
        else:
            # å•å¡è¿è¡Œï¼Œæ‰‹åŠ¨è®¾ç½®è™šæ‹Ÿç¯å¢ƒ
            print("ğŸ”§ æœªæ£€æµ‹åˆ°å¯åŠ¨å™¨ç¯å¢ƒï¼Œä¸ºå•å¡é‡‡æ ·åˆå§‹åŒ–å•è¿›ç¨‹ç»„...")
            os.environ["MASTER_ADDR"] = "localhost"
            os.environ["MASTER_PORT"] = "12355"
            os.environ["RANK"] = "0"
            os.environ["WORLD_SIZE"] = "1"
            torch.distributed.init_process_group(backend="gloo", rank=0, world_size=1)
            print("âœ… å•è¿›ç¨‹ç»„åˆå§‹åŒ–æˆåŠŸã€‚")
    
    
    # GPUè®¾ç½®
    use_gpu = cfg.general.gpus > 0 and torch.cuda.is_available()
    device = torch.device("cuda:0" if use_gpu else "cpu")
    
    # 1. åˆ›å»ºæ•°æ®æ¨¡å—å’Œæ¨¡å‹ç»„ä»¶ (ä¸è®­ç»ƒæ—¶ç›¸åŒ)
    try:
        datamodule, model_kwargs = create_datamodule_and_components(cfg)
        print("âœ… æ•°æ®æ¨¡å—å’Œæ¨¡å‹ç»„ä»¶åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ•°æ®æ¨¡å—å’Œæ¨¡å‹ç»„ä»¶åˆ›å»ºå¤±è´¥: {e}")
        raise e
    
    try:
    # device = torch.device(f"cuda:0" if use_gpu else "cpu") # Trainerä¼šè‡ªåŠ¨å¤„ç†è®¾å¤‡
    # å°† trainer çš„ total_steps ä¼ é€’ä¸‹å»
        grpo_module = create_grpo_lightning_module(
            cfg=cfg,
            model_kwargs=model_kwargs,
            datamodule=datamodule,
            total_steps=cfg.grpo.total_steps,
        )
        print("âœ… GRPO Lightningæ¨¡å—åˆ›å»ºæˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ GRPO Lightningæ¨¡å—åˆ›å»ºå¤±è´¥: {e}")
        raise e    
    # 2. ä½¿ç”¨Lightningçš„æ ‡å‡†æ–¹å¼ä»checkpointåŠ è½½æ¨¡å‹
    checkpoint = torch.load(checkpoint_path, map_location='cpu')

    # --- é‡æ˜ å°„ state_dict çš„é”® ---
    # é¢„è®­ç»ƒæ¨¡å‹æ˜¯ GraphDiscreteFlowModelï¼Œè€Œæˆ‘ä»¬è¦åŠ è½½åˆ° GRPOLightningModule ä¸­ï¼Œ
    # GRPOLightningModule å°† GraphDiscreteFlowModel å­˜åœ¨ self.model å±æ€§ä¸‹ã€‚
    # å› æ­¤ï¼Œé¢„è®­ç»ƒçš„é”® (e.g., 'model.layers.0.weight') éœ€è¦è¢«æ˜ å°„ä¸º
    # 'model.model.layers.0.weight' æ‰èƒ½æ­£ç¡®åŠ è½½åˆ°DDPåŒ…è£…çš„æ¨¡å‹ä¸­ã€‚
    print("ğŸ”§ æ­£åœ¨é‡æ˜ å°„æ£€æŸ¥ç‚¹é”®åä»¥åŒ¹é…æ¨¡å‹ç»“æ„...")
    remapped_state_dict = {}
    for k, v in checkpoint['state_dict'].items():
        # ä¸ºæ‰€æœ‰æ¥è‡ªåŸå§‹ state_dict çš„é”®æ·»åŠ  'model.' å‰ç¼€
        new_key = f"model.{k}"
        remapped_state_dict[new_key] = v
    print("âœ… é”®åé‡æ˜ å°„å®Œæˆã€‚")
    
    grpo_module.load_state_dict(remapped_state_dict, strict=False)
    print("âœ… é¢„è®­ç»ƒæƒé‡å·²åŠ è½½åˆ°æ¨¡å‹ã€‚")
 
     # --- å°†æ¨¡å‹ç§»åŠ¨åˆ°GPU ---
    if use_gpu:
        print(f"ğŸš€ å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡: {device}...")
        grpo_module = grpo_module.to(device)
        print("âœ… æ¨¡å‹å·²æˆåŠŸç§»åŠ¨åˆ°GPUã€‚")
     # --- GPUç§»åŠ¨ç»“æŸ ---
          
      # 3. æ‰‹åŠ¨åˆå§‹åŒ–GRPOç»„ä»¶ (å¥–åŠ±å‡½æ•°ç­‰)
    grpo_module.setup("fit")
      
      # 4. å…³é”®ï¼šå°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    grpo_module.eval()
    print("âœ… æ¨¡å‹å·²è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ (model.eval())")
    
    # 5. è·å–é‡‡æ ·é…ç½®
    num_samples = cfg.grpo.get('num_samples_to_validate', 32)
    batch_size = cfg.grpo.group_size
    print(f"ğŸ“ é‡‡æ ·é…ç½®: æ€»æ ·æœ¬æ•°={num_samples}, æ¯æ‰¹æ¬¡å¤§å°={batch_size}")
    
    # 6. æ‰§è¡Œé‡‡æ ·å’Œè¯„ä¼°
    all_rewards = []
    num_batches = 1
    
    for i in range(num_batches):
        current_batch_size = min(batch_size, num_samples - len(all_rewards))
        if current_batch_size <= 0:
            break
            
        print(f"\n--- æ­£åœ¨é‡‡æ ·æ‰¹æ¬¡ {i+1}/{num_batches}, æ•°é‡: {current_batch_size} ---")
        
        # ç›´æ¥è°ƒç”¨ GRPOLightningModule ä¸Šçš„éªŒè¯æ–¹æ³•
        metrics = grpo_module.validate_pure_sampling(
            batch_size=current_batch_size,
            seed=cfg.train.seed + i,  # ä¸ºæ¯ä¸ªæ‰¹æ¬¡ä½¿ç”¨ä¸åŒçš„ç§å­
            save_samples=True      # ä¿å­˜æ ·æœ¬ä»¥ä¾›åˆ†æ
        )
        
        if 'error' not in metrics:
            print(f"   æ‰¹æ¬¡å¹³å‡å¥–åŠ±: {metrics.get('average_reward', 0):.6f}")
        else:
            print(f"   æ‰¹æ¬¡é‡‡æ ·å¤±è´¥: {metrics['error']}")

    print("\n" + "="*60)
    print("ğŸ‰ é‡‡æ ·å’Œè¯„ä¼°å®Œæˆï¼")
    print("="*60)


@hydra.main(version_base="1.3", config_path="../configs", config_name="grpo_lightning_config")
def main(cfg: DictConfig):
    """
    GRPO Lightningè®­ç»ƒçš„ä¸»å…¥å£å‡½æ•°
    ä¸åŸå§‹main.pyä¿æŒç›¸åŒçš„ç»“æ„ï¼Œæ”¯æŒè®­ç»ƒå’Œæµ‹è¯•æ¨¡å¼
    """
    try:
        # å†³å®šè¿è¡Œæ¨¡å¼
        if cfg.grpo.get('sample_only', None):
            # æ–°å¢çš„çº¯é‡‡æ ·æ¨¡å¼
            checkpoint_path = cfg.grpo.sample_only
            if not os.path.exists(checkpoint_path):
                raise FileNotFoundError(f"é‡‡æ ·checkpointä¸å­˜åœ¨: {checkpoint_path}")
            
            run_grpo_lightning_sampling(cfg, checkpoint_path)
            
        elif getattr(cfg.grpo, 'test_only', None):
            # æµ‹è¯•æ¨¡å¼ (ä¿ç•™æ—§çš„test_onlyé€»è¾‘)
            checkpoint_path = cfg.grpo.test_only
            if not os.path.exists(checkpoint_path):
                raise FileNotFoundError(f"æµ‹è¯•checkpointä¸å­˜åœ¨: {checkpoint_path}")
            
            run_grpo_lightning_testing(cfg, checkpoint_path)
            
        else:
            # è®­ç»ƒæ¨¡å¼
            grpo_module = run_grpo_lightning_training(cfg)
        
        print("ğŸ‰ GRPO Lightningæ‰§è¡Œå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ GRPO Lightningæ‰§è¡Œå¤±è´¥: {e}")
        raise e


if __name__ == "__main__":
    main() 