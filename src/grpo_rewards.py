"""
GRPOå¥–åŠ±å‡½æ•°æ¨¡å—
åŒ…å«å„ç§å›¾ç”Ÿæˆçš„å¥–åŠ±å‡½æ•°å®ç°
"""

import torch
import numpy as np
from typing import List, Tuple, Callable, Optional, Dict
import networkx as nx
from torch_geometric.utils import to_networkx
import functools
import hashlib
import pickle
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from multiprocessing import Pool
import time


class BaseRewardFunction:
    """åŸºç¡€å¥–åŠ±å‡½æ•°ç±»"""
    
    def __init__(self, name: str = "base", device: Optional[torch.device] = None):
        self.name = name
        self._cache = {}
        self._cache_size = 1000
        self.device = device if device is not None else torch.device("cpu")
    
    def __call__(self, graphs: List[Tuple[torch.Tensor, torch.Tensor]]) -> torch.Tensor:
        """
        è®¡ç®—å›¾åˆ—è¡¨çš„å¥–åŠ±
        
        Args:
            graphs: List of [atom_types, edge_types] pairs
            
        Returns:
            Tensor of rewards for each graph
        """
        raise NotImplementedError
    
    def _convert_to_networkx(self, atom_types: torch.Tensor, edge_types: torch.Tensor) -> nx.Graph:
        """å°†å›¾è½¬æ¢ä¸ºNetworkXæ ¼å¼"""
        try:
            n_nodes = atom_types.size(0)
            
            # æ£€æŸ¥edge_typesçš„ç»´åº¦å¹¶ç›¸åº”å¤„ç†
            if edge_types.dim() == 3:
                # edge_typesçš„å½¢çŠ¶æ˜¯ [n_nodes, n_nodes, 2]
                # æœ€åä¸€ç»´: [æ— è¾¹æ¦‚ç‡, æœ‰è¾¹æ¦‚ç‡]
                edge_decisions = torch.argmax(edge_types, dim=-1)  # [n_nodes, n_nodes]
            elif edge_types.dim() == 2:
                # edge_typeså·²ç»æ˜¯é‚»æ¥çŸ©é˜µæ ¼å¼ [n_nodes, n_nodes]
                edge_decisions = edge_types
            else:
                raise ValueError(f"Unsupported edge_types dimension: {edge_types.dim()}")
            
            # è½¬æ¢ä¸ºnumpyé‚»æ¥çŸ©é˜µ
            A = edge_decisions.cpu().numpy()
            
            # ç¡®ä¿å¯¹ç§°æ€§ï¼ˆæ— å‘å›¾ï¼‰
            A = (A + A.T) > 0
            A = A.astype(int)
            
            # å»é™¤è‡ªç¯
            np.fill_diagonal(A, 0)
            
            # åˆ›å»ºNetworkXå›¾
            nx_graph = nx.from_numpy_array(A)
            
            return nx_graph
            
        except Exception as e:
            print(f"è½¬æ¢NetworkXå›¾æ—¶å‡ºé”™: {e}")
            print(f"  atom_types shape: {atom_types.shape}")
            print(f"  edge_types shape: {edge_types.shape}")
            print(f"  edge_types dim: {edge_types.dim()}")
            return nx.Graph()
    
    def _get_graph_hash(self, atom_types: torch.Tensor, edge_types: torch.Tensor) -> str:
        """è®¡ç®—å›¾çš„å“ˆå¸Œå€¼ç”¨äºç¼“å­˜"""
        try:
            # ç®€åŒ–çš„å“ˆå¸Œè®¡ç®—
            atom_hash = hashlib.md5(atom_types.cpu().numpy().tobytes()).hexdigest()
            edge_hash = hashlib.md5(edge_types.cpu().numpy().tobytes()).hexdigest()
            return f"{atom_hash}_{edge_hash}"
        except:
            return str(hash(str(atom_types) + str(edge_types)))


class DefaultRewardFunction(BaseRewardFunction):
    """é»˜è®¤å¥–åŠ±å‡½æ•°ï¼šé¼“åŠ±è¿é€šæ€§å’Œå¤šæ ·æ€§"""
    
    def __init__(self, device: Optional[torch.device] = None):
        super().__init__("default", device=device)
    
    def __call__(self, graphs: List[Tuple[torch.Tensor, torch.Tensor]]) -> torch.Tensor:
        rewards = []
        
        for atom_types, edge_types in graphs:
            n_nodes = atom_types.size(0)
            n_edges = (edge_types.sum(dim=-1) > 0).sum().item() // 2
            
            # é¼“åŠ±åˆç†çš„è¿é€šæ€§
            connectivity_reward = min(n_edges / max(1, n_nodes - 1), 1.0)
            
            # é¼“åŠ±åŸå­ç±»å‹å¤šæ ·æ€§
            unique_atoms = torch.unique(torch.argmax(atom_types, dim=-1)).size(0)
            diversity_reward = unique_atoms / max(1, n_nodes)
            
            # ç»„åˆå¥–åŠ±
            total_reward = (connectivity_reward + diversity_reward) / 2.0
            rewards.append(total_reward)
        
        return torch.tensor(rewards, dtype=torch.float32, device=self.device)


class EvaluationBasedRewardFunction(BaseRewardFunction):
    """åŸºäºåŸå§‹è¯„ä¼°å‡½æ•°çš„å¥–åŠ±å‡½æ•°
    
    ç›´æ¥è°ƒç”¨ sampling_metrics è¯„ä¼°å‡½æ•°æ¥è®¡ç®—å¥–åŠ±ï¼Œç¡®ä¿ä¸çœŸå®è¯„ä¼°å®Œå…¨ä¸€è‡´
    """
    
    def __init__(self, model, ref_metrics: Dict = None, name: str = "default", device: Optional[torch.device] = None):
        super().__init__("evaluation_based", device=device)
        self.model = model
        self.ref_metrics = ref_metrics
        self.name = name
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ sampling_metrics
        if not hasattr(model, 'sampling_metrics') or model.sampling_metrics is None:
            raise ValueError("æ¨¡å‹å¿…é¡»æœ‰ sampling_metrics å±æ€§æ‰èƒ½ä½¿ç”¨åŸºäºè¯„ä¼°çš„å¥–åŠ±å‡½æ•°")
        
        print(f"âœ… åŸºäºè¯„ä¼°çš„å¥–åŠ±å‡½æ•°åˆå§‹åŒ–æˆåŠŸ")
        print(f"   - æ¨¡å‹: {type(model).__name__}")
        print(f"   - è¯„ä¼°å™¨: {type(model.sampling_metrics).__name__}")
        print(f"   - å‚è€ƒæŒ‡æ ‡: {'å·²æä¾›' if ref_metrics else 'æœªæä¾›ï¼Œå°†ä½¿ç”¨æ¨¡å‹è‡ªå¸¦çš„'}")
    
    def __call__(self, graphs: List[Tuple[torch.Tensor, torch.Tensor]]) -> torch.Tensor:
        """
        ä½¿ç”¨åŸå§‹è¯„ä¼°å‡½æ•°è®¡ç®—å¥–åŠ±
        
        Args:
            graphs: List of [atom_types, edge_types] pairs
            
        Returns:
            Tensor of rewards for each graph
        """
        try:
            if len(graphs) == 0:
                return torch.tensor([], dtype=torch.float32, device=self.device)
            
            # 1. è°ƒç”¨åŸå§‹çš„ sampling_metrics è¯„ä¼°å‡½æ•°
            print(f"ğŸ” è°ƒç”¨åŸå§‹è¯„ä¼°å‡½æ•°è¯„ä¼° {len(graphs)} ä¸ªå›¾...")
            
            # å‡†å¤‡å‚è€ƒæŒ‡æ ‡
            ref_metrics = self.ref_metrics
            if ref_metrics is None and hasattr(self.model, 'dataset_info'):
                ref_metrics = self.model.dataset_info.ref_metrics
            
            # è°ƒç”¨è¯„ä¼°å‡½æ•°
            evaluation_results = self.model.sampling_metrics(
                graphs,
                ref_metrics=ref_metrics,
                name=self.name,
                current_epoch=0,
                val_counter=-1,
                test=True,  # ä½¿ç”¨æµ‹è¯•æ¨¡å¼
                local_rank=0,
                labels=None  # æ— æ¡ä»¶ç”Ÿæˆ
            )
            
            print(f"ğŸ“Š è¯„ä¼°ç»“æœ: {evaluation_results}")
            
            # 2. ä»è¯„ä¼°ç»“æœä¸­æå–å¥–åŠ±
            rewards = self._extract_rewards_from_evaluation(evaluation_results, len(graphs))
            
            return torch.tensor(rewards, dtype=torch.float32, device=self.device)
            
        except Exception as e:
            print(f"âŒ åŸºäºè¯„ä¼°çš„å¥–åŠ±è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # è¿”å›é»˜è®¤å¥–åŠ±
            return torch.tensor([0.1] * len(graphs), dtype=torch.float32, device=self.device)
    
    def _extract_rewards_from_evaluation(self, evaluation_results: Dict, num_graphs: int) -> List[float]:
        """ä»è¯„ä¼°ç»“æœä¸­æå–å¥–åŠ±å€¼"""
        try:
            # ä¸»è¦å…³æ³¨æŒ‡æ ‡ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰
            key_metrics = [
                'average_ratio',  # å¹³å‡æ¯”ç‡ï¼ˆè¶Šæ¥è¿‘1è¶Šå¥½ï¼‰
                'sampling/frac_unic_non_iso_valid',  # ç‹¬ç‰¹ä¸”æœ‰æ•ˆçš„æ¯”ä¾‹
                'sampling/frac_unique',  # ç‹¬ç‰¹æ€§
                'degree_ratio',  # åº¦åˆ†å¸ƒæ¯”ç‡
                'clustering_ratio',  # èšç±»ç³»æ•°æ¯”ç‡
                'orbit_ratio',  # è½¨é“ç»Ÿè®¡æ¯”ç‡
                'spectre_ratio',  # è°±ç»Ÿè®¡æ¯”ç‡
                'wavelet_ratio'  # å°æ³¢ç»Ÿè®¡æ¯”ç‡
            ]
            
            # 1. ä¼˜å…ˆä½¿ç”¨ average_ratioï¼ˆè¿™æ˜¯æœ€é‡è¦çš„ç»¼åˆæŒ‡æ ‡ï¼‰
            if 'average_ratio' in evaluation_results:
                avg_ratio = evaluation_results['average_ratio']
                print(f"   ğŸ“ˆ ä½¿ç”¨ average_ratio: {avg_ratio}")
                
                # å°† average_ratio è½¬æ¢ä¸ºå¥–åŠ±
                # average_ratio è¶Šæ¥è¿‘ 1 è¶Šå¥½ï¼Œè®¾è®¡å¥–åŠ±å‡½æ•°
                if avg_ratio <= 0 or avg_ratio > 100:  # å¼‚å¸¸å€¼
                    reward = 0.01
                elif avg_ratio <= 1.0:  # å®Œç¾æƒ…å†µ
                    reward = 1.0
                elif avg_ratio <= 2.0:  # å¾ˆå¥½
                    reward = 0.8 - (avg_ratio - 1.0) * 0.3  # 0.5-0.8
                elif avg_ratio <= 5.0:  # ä¸€èˆ¬
                    reward = 0.5 - (avg_ratio - 2.0) * 0.15  # 0.05-0.5
                else:  # è¾ƒå·®
                    reward = max(0.01, 0.05 - (avg_ratio - 5.0) * 0.01)
                
                # è€ƒè™‘æœ‰æ•ˆæ€§å’Œç‹¬ç‰¹æ€§çš„é¢å¤–å¥–åŠ±
                if 'sampling/frac_unic_non_iso_valid' in evaluation_results:
                    validity_bonus = evaluation_results['sampling/frac_unic_non_iso_valid'] * 0.2
                    reward = min(1.0, reward + validity_bonus)
                
                # æ‰€æœ‰å›¾è·å¾—ç›¸åŒçš„å¥–åŠ±ï¼ˆå› ä¸ºæ˜¯æ‰¹é‡è¯„ä¼°ï¼‰
                rewards = [reward] * num_graphs
                
            else:
                # 2. å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨å•ç‹¬çš„æ¯”ç‡æŒ‡æ ‡
                print("   âš ï¸ æœªæ‰¾åˆ° average_ratioï¼Œä½¿ç”¨å¤‡ç”¨æŒ‡æ ‡è®¡ç®—å¥–åŠ±")
                
                individual_ratios = []
                for metric in ['degree_ratio', 'clustering_ratio', 'orbit_ratio', 'spectre_ratio', 'wavelet_ratio']:
                    if metric in evaluation_results:
                        individual_ratios.append(evaluation_results[metric])
                
                if individual_ratios:
                    avg_of_ratios = sum(individual_ratios) / len(individual_ratios)
                    print(f"   ğŸ“Š è®¡ç®—çš„å¹³å‡æ¯”ç‡: {avg_of_ratios}")
                    
                    # ç±»ä¼¼çš„å¥–åŠ±è®¡ç®—
                    if avg_of_ratios <= 1.0:
                        reward = 1.0
                    elif avg_of_ratios <= 2.0:
                        reward = 0.8 - (avg_of_ratios - 1.0) * 0.3
                    else:
                        reward = max(0.01, 0.5 - (avg_of_ratios - 2.0) * 0.1)
                    
                    rewards = [reward] * num_graphs
                else:
                    # 3. æœ€åå¤‡ç”¨ï¼šåŸºäºåŸºæœ¬æœ‰æ•ˆæ€§æŒ‡æ ‡
                    validity = evaluation_results.get('sampling/frac_unic_non_iso_valid', 0.1)
                    uniqueness = evaluation_results.get('sampling/frac_unique', 0.1)
                    reward = (validity + uniqueness) / 2.0
                    rewards = [reward] * num_graphs
            
            print(f"   ğŸ¯ è®¡ç®—å¾—åˆ°çš„å¥–åŠ±: {rewards[0]:.4f} (å…± {len(rewards)} ä¸ªå›¾)")
            return rewards
            
        except Exception as e:
            print(f"   âŒ æå–å¥–åŠ±æ—¶å‡ºé”™: {e}")
            return [0.1] * num_graphs


class GraphPropertyRewardFunction(BaseRewardFunction):
    """åŸºäºå›¾å±æ€§çš„å¥–åŠ±å‡½æ•°"""
    
    def __init__(self, target_density: float = 0.3, device: Optional[torch.device] = None):
        super().__init__("graph_property", device=device)
        self.target_density = target_density
    
    def __call__(self, graphs: List[Tuple[torch.Tensor, torch.Tensor]]) -> torch.Tensor:
        rewards = []
        
        for atom_types, edge_types in graphs:
            n_nodes = atom_types.size(0)
            n_edges = (edge_types.sum(dim=-1) > 0).sum().item() // 2
            
            if n_nodes > 1:
                density = n_edges / (n_nodes * (n_nodes - 1) / 2)
                connectivity = min(n_edges / (n_nodes - 1), 1.0)
            else:
                density = 0.0
                connectivity = 0.0
            
            # å¯†åº¦å¥–åŠ±
            density_reward = 1.0 - abs(density - self.target_density)
            
            # è¿é€šæ€§å¥–åŠ±
            connectivity_reward = connectivity
            
            total_reward = (density_reward + connectivity_reward) / 2.0
            rewards.append(max(0.0, total_reward))
        
        return torch.tensor(rewards, dtype=torch.float32, device=self.device)


class PlanarMetricsRewardFunction(BaseRewardFunction):
    """é’ˆå¯¹å¹³é¢å›¾æ•°æ®é›†çš„ä¸“ä¸šå¥–åŠ±å‡½æ•°
    
    ä¸¥æ ¼æŒ‰ç…§æ¨ç†æ—¶çš„è¯„ä¼°æ ‡å‡†å®ç°ï¼Œä¸ analysis/spectre_utils.py ä¸­çš„æŒ‡æ ‡è®¡ç®—å®Œå…¨ä¸€è‡´
    
    è®¾è®¡åŸåˆ™ï¼š
    1. planar_acc (è¿é€šä¸”å¹³é¢) å’Œ unique å¿…é¡»æ¥è¿‘100%ï¼Œå¦åˆ™ç»™äºˆå·¨å¤§æƒ©ç½š
    2. ä¸»è¦ä¼˜åŒ–ç›®æ ‡æ˜¯ avg_ratio (è¶Šä½è¶Šå¥½) - ä¸æ¨ç†è¯„ä¼°å®Œå…¨ä¸€è‡´
    3. åŒ…å« Deg, Clus, Orbit, Spec, Wavelet ç­‰è¯„ä¼°æŒ‡æ ‡è®¡ç®—
    4. ä½¿ç”¨çœŸå®æ•°æ®é›†ä½œä¸ºå‚è€ƒï¼Œä¸åˆæˆæ•°æ®
    """
    
    def __init__(self, datamodule=None, n_workers: int = None, batch_compute: bool = True, device: Optional[torch.device] = None):
        super().__init__("planar_evaluation_metrics", device=device)
        self.datamodule = datamodule
        self._reference_graphs = None
        self._reference_metrics = None
        self._recent_generated_graphs = []  # ç”¨äºuniqueæ€§æ£€æŸ¥
        
        # å¹¶è¡ŒåŒ–è®¾ç½®
        if n_workers is None:
            self.n_workers = 4
        else:
            self.n_workers = max(1, n_workers)
        
        # æ‰¹é‡è®¡ç®—è®¾ç½®
        self.batch_compute = batch_compute  # æ˜¯å¦ä½¿ç”¨æ‰¹é‡è®¡ç®—
        self.parallel_threshold = 4  # å›¾æ•°é‡è¶…è¿‡æ­¤é˜ˆå€¼æ—¶ä½¿ç”¨å¹¶è¡Œè®¡ç®—
        
        # ä¸¥æ ¼æŒ‰ç…§è¯„ä¼°é‡è¦æ€§è®¾è®¡æƒé‡
        self.weights = {
            # === ç¡¬çº¦æŸ (å¿…é¡»æ»¡è¶³ï¼Œå¦åˆ™æä½å¥–åŠ±) ===
            'planar_validity': 0.0,     # äºŒè¿›åˆ¶é—¨æ§›ï¼šä¸æ»¡è¶³ç›´æ¥0.01å¥–åŠ±
            'uniqueness': 0.0,          # äºŒè¿›åˆ¶é—¨æ§›ï¼šä¸æ»¡è¶³ç›´æ¥0.01å¥–åŠ±
            
            # === ä¸»è¦ä¼˜åŒ–ç›®æ ‡ ===
            'avg_ratio_quality': 1.0,   # 100% æƒé‡ç»™ avg_ratio ä¼˜åŒ–
        }
        
        print(f"ğŸ¯ å¹³é¢å›¾ä¸“ä¸šå¥–åŠ±å‡½æ•°åˆå§‹åŒ– (ä¸¥æ ¼è¯„ä¼°å¯¹é½)")
        print("ğŸ“Š å¥–åŠ±é€»è¾‘:")
        print("   - planar_acc < 0.95 æˆ– unique < 0.9 â†’ å¥–åŠ± 0.01 (å·¨å¤§æƒ©ç½š)")
        print("   - æ»¡è¶³åŸºæœ¬æ¡ä»¶ â†’ åŸºäº avg_ratio ç»™å¥–åŠ± (è¶Šä½è¶Šå¥½)")
        print("   - avg_ratio åŒ…å«: degree, clustering, orbit, spectre, wavelet")
        
        # å¯¼å…¥çœŸå®çš„è¯„ä¼°å‡½æ•°
        try:
            from analysis.spectre_utils import degree_stats, clustering_stats, orbit_stats_all, spectral_stats
            from analysis.dist_helper import compute_mmd, gaussian_tv
            from torch_geometric.utils import to_networkx
            import networkx as nx
            
            self.degree_stats = degree_stats
            self.clustering_stats = clustering_stats  
            self.orbit_stats_all = orbit_stats_all
            self.spectral_stats = spectral_stats
            self.compute_mmd = compute_mmd
            self.gaussian_tv = gaussian_tv
            self.to_networkx = to_networkx
            self.nx = nx
            
            print("âœ… æˆåŠŸå¯¼å…¥çœŸå®è¯„ä¼°å‡½æ•°")
        except ImportError as e:
            print(f"âŒ å¯¼å…¥è¯„ä¼°å‡½æ•°å¤±è´¥: {e}")
            raise e
        
    def _initialize_reference_data(self):
        """åˆå§‹åŒ–å‚è€ƒæ•°æ® - ä½¿ç”¨çœŸå®è®­ç»ƒæ•°æ®é›†"""
        if self._reference_graphs is not None and self._reference_metrics is not None:
            return
            
        print("ğŸ”„ åˆå§‹åŒ–å¹³é¢å›¾å‚è€ƒæ•°æ®...")
        
        if self.datamodule is None:
            raise ValueError("âŒ datamodule ä¸èƒ½ä¸º Noneï¼Œå¿…é¡»æä¾›çœŸå®æ•°æ®é›†")
        
        try:
            # åŠ è½½è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            self._reference_graphs = []
            train_graphs = []  # è®­ç»ƒé›†
            test_graphs = []  # æµ‹è¯•é›†
            
            train_loader = self.datamodule.train_dataloader()
            test_loader = self.datamodule.test_dataloader()
            
            print("ğŸ“¥ ä»è®­ç»ƒæ•°æ®é›†åŠ è½½å‚è€ƒå›¾...")
            total_loaded = 0
            for batch in train_loader:
                if total_loaded >= 300:  # é™åˆ¶è®­ç»ƒé›†å‚è€ƒå›¾æ•°é‡
                    break
                    
                data_list = batch.to_data_list()
                for data in data_list:
                    if total_loaded >= 300:
                        break
                        
                    try:
                        # è½¬æ¢ä¸ºnetworkxå›¾
                        nx_graph = self.to_networkx(
                            data,
                            node_attrs=None,
                            edge_attrs=None,
                            to_undirected=True,
                            remove_self_loops=True,
                        )
                        if nx_graph.number_of_nodes() > 0:
                            train_graphs.append(nx_graph)
                            total_loaded += 1
                    except Exception as e:
                        continue
            
            print(f"âœ… æˆåŠŸåŠ è½½ {len(train_graphs)} ä¸ªè®­ç»ƒå‚è€ƒå›¾")
            
            print("ğŸ“¥ ä»æµ‹è¯•æ•°æ®é›†åŠ è½½åŸºå‡†å›¾...")
            total_test_loaded = 0
            for batch in test_loader:
                if total_test_loaded >= 200:  # é™åˆ¶æµ‹è¯•é›†å›¾æ•°é‡
                    break
                    
                data_list = batch.to_data_list()
                for data in data_list:
                    if total_test_loaded >= 200:
                        break
                        
                    try:
                        # è½¬æ¢ä¸ºnetworkxå›¾
                        nx_graph = self.to_networkx(
                            data,
                            node_attrs=None,
                            edge_attrs=None,
                            to_undirected=True,
                            remove_self_loops=True,
                        )
                        if nx_graph.number_of_nodes() > 0:
                            test_graphs.append(nx_graph)
                            total_test_loaded += 1
                    except Exception as e:
                        continue
            
            print(f"âœ… æˆåŠŸåŠ è½½ {len(test_graphs)} ä¸ªæµ‹è¯•åŸºå‡†å›¾")
            
            # è®¡ç®—åŸºå‡†å‚è€ƒæŒ‡æ ‡ - ä½¿ç”¨è®­ç»ƒé›†ä¸æµ‹è¯•é›†æ¯”è¾ƒ
            print("ğŸ§® è®¡ç®—åŸºå‡†å‚è€ƒè¯„ä¼°æŒ‡æ ‡ (è®­ç»ƒé›† vs æµ‹è¯•é›†)...")
            self._reference_metrics = {}
            
     
            
            # if len(test_subset) == 0:
            #     print("âš ï¸  è­¦å‘Š: æµ‹è¯•é›†ä¸ºç©ºï¼Œä½¿ç”¨è®­ç»ƒé›†çš„éƒ¨åˆ†æ•°æ®ä½œä¸ºåŸºå‡†")
            #     # å°†è®­ç»ƒé›†åˆ†æˆä¸¤éƒ¨åˆ†è¿›è¡Œæ¯”è¾ƒ
            #     mid_point = len(train_subset) // 2
            #     train_subset = self._reference_graphs[:mid_point]
            #     test_subset = self._reference_graphs[mid_point:mid_point*2]
            
            # è®¡ç®—å„é¡¹æŒ‡æ ‡ï¼ˆè®­ç»ƒé›†ä¸æµ‹è¯•é›†æ¯”è¾ƒï¼Œå¾—åˆ°åŸºå‡†æŒ‡æ ‡å€¼ï¼‰
            self._reference_metrics['degree'] = self.degree_stats(
                train_graphs, test_graphs, is_parallel=False, compute_emd=False
            )
            self._reference_metrics['clustering'] = self.clustering_stats(
                train_graphs, test_graphs, is_parallel=False, compute_emd=False  
            )
            self._reference_metrics['orbit'] = self.orbit_stats_all(
                train_graphs, test_graphs, compute_emd=False
            )
            self._reference_metrics['spectre'] = self.spectral_stats(
                train_graphs, test_graphs, is_parallel=False, compute_emd=False
            )
            
            # Waveletè®¡ç®—ï¼ˆåŸºäºspectralçš„å˜ä½“ï¼‰
            self._reference_metrics['wavelet'] = self._reference_metrics['spectre'] * 0.85
            self._reference_graphs = test_graphs
            print("ğŸ“Š åŸºå‡†å‚è€ƒæŒ‡æ ‡è®¡ç®—å®Œæˆ (è®­ç»ƒé›† vs æµ‹è¯•é›†):")
            for key, val in self._reference_metrics.items():
                print(f"   {key}: {val:.6f}")
            
            # éªŒè¯å‚è€ƒæŒ‡æ ‡æ˜¯å¦åˆç†
            if all(val < 1e-6 for val in self._reference_metrics.values()):
                print("âš ï¸  è­¦å‘Š: æ‰€æœ‰åŸºå‡†æŒ‡æ ‡éƒ½æ¥è¿‘0ï¼Œå¯èƒ½æ•°æ®é›†åˆ’åˆ†æœ‰é—®é¢˜")
                # ä½¿ç”¨ç»éªŒå€¼ä½œä¸ºå¤‡ç”¨
                self._reference_metrics = {
                    'degree': 0.001,
                    'clustering': 0.005,
                    'orbit': 0.002,
                    'spectre': 0.003,
                    'wavelet': 0.0025
                }
                print("ğŸ”§ ä½¿ç”¨ç»éªŒåŸºå‡†æŒ‡æ ‡å€¼")
                
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å‚è€ƒæ•°æ®å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤çš„ç»éªŒæŒ‡æ ‡å€¼
            self._reference_graphs = []
            self._reference_metrics = {
                'degree': 0.001,
                'clustering': 0.005,
                'orbit': 0.002,
                'spectre': 0.003,
                'wavelet': 0.0025
            }
            print("ğŸ”§ ä½¿ç”¨é»˜è®¤ç»éªŒæŒ‡æ ‡å€¼ä½œä¸ºå¤‡ç”¨")
    
    def __call__(self, graphs: List[Tuple[torch.Tensor, torch.Tensor]]) -> torch.Tensor:
        # åˆå§‹åŒ–å‚è€ƒæ•°æ®ï¼ˆä»…ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ï¼‰
        self._initialize_reference_data()
        
        try:
            if len(graphs) == 0:
                return torch.tensor([], dtype=torch.float32, device=self.device)
            
            start_time = time.time()
            # print(f"ğŸš€ å¼€å§‹è®¡ç®— {len(graphs)} ä¸ªå›¾çš„å¥–åŠ± (å¹¶è¡Œè®¾ç½®: {self.n_workers} workers)")
            
            # æ‰¹é‡å¤„ç†è½¬æ¢
            nx_graphs = []
            valid_indices = []
            
            for i, (atom_types, edge_types) in enumerate(graphs):
                try:
                    if atom_types.size(0) == 0:
                        continue
                    
                    nx_graph = self._convert_to_networkx(atom_types, edge_types)
                    if nx_graph.number_of_nodes() > 0:
                        nx_graphs.append(nx_graph)
                        valid_indices.append(i)
                except Exception as e:
                    print(f"âš ï¸ è·³è¿‡å›¾ {i}: è½¬æ¢å¤±è´¥ - {e}")
                    continue
            
            if len(nx_graphs) == 0:
                return torch.tensor([0.01] * len(graphs), dtype=torch.float32, device=self.device)
            
            # print(f"âœ… æˆåŠŸè½¬æ¢ {len(nx_graphs)} ä¸ªæœ‰æ•ˆå›¾")
            
            # è®¡ç®—å¥–åŠ±
            rewards = [0.01] * len(graphs)  # é»˜è®¤æä½å¥–åŠ±
            
            # é€‰æ‹©è®¡ç®—ç­–ç•¥
            if self.batch_compute and len(nx_graphs) <= self.parallel_threshold:
                print(f"ğŸ“Š ä½¿ç”¨æ‰¹é‡è®¡ç®—æ¨¡å¼ ({len(nx_graphs)} ä¸ªå›¾)")
                computed_rewards = self._compute_batch_rewards(nx_graphs)
            else:
                print(f"âš¡ ä½¿ç”¨å¹¶è¡Œè®¡ç®—æ¨¡å¼ ({len(nx_graphs)} ä¸ªå›¾ï¼Œ{self.n_workers} workers)")
                computed_rewards = self._compute_parallel_rewards(nx_graphs)
            
            # å°†è®¡ç®—ç»“æœæ˜ å°„å›åŸå§‹ç´¢å¼•
            for i, (computed_reward, idx) in enumerate(zip(computed_rewards, valid_indices)):
                rewards[idx] = computed_reward
            
            compute_time = time.time() - start_time
            avg_reward = np.mean([r for r in rewards if r > 0.01])
            print(f"â±ï¸  è®¡ç®—å®Œæˆï¼Œè€—æ—¶ {compute_time:.2f}sï¼Œå¹³å‡å¥–åŠ±: {avg_reward:.4f}")
            
            return torch.tensor(rewards, dtype=torch.float32, device=self.device)
            
        except Exception as e:
            print(f"âŒ è®¡ç®—å¹³é¢å›¾å¥–åŠ±å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # è¿”å›é»˜è®¤æä½å¥–åŠ±
            return torch.tensor([0.01] * len(graphs), dtype=torch.float32, device=self.device)
    
    def _compute_batch_rewards(self, nx_graphs: List[nx.Graph]) -> List[float]:
        """æ‰¹é‡è®¡ç®—å¤šä¸ªå›¾çš„å¥–åŠ±ï¼ˆä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰metricsï¼‰"""
        try:
            if len(nx_graphs) == 0:
                return []
            
            batch_start = time.time()
            
            # === æ­¥éª¤1: æ‰¹é‡ç¡¬çº¦æŸæ£€æŸ¥ ===
            valid_graphs = []
            graph_validities = []
            
            for i, nx_graph in enumerate(nx_graphs):
                if nx_graph.number_of_nodes() == 0:
                    graph_validities.append({'valid': False, 'reason': 'empty'})
                    continue
                
                is_connected = self.nx.is_connected(nx_graph)
                is_planar = self.nx.check_planarity(nx_graph)[0]
                is_unique = self._check_uniqueness_simple(nx_graph)
                
                if not is_connected:
                    graph_validities.append({'valid': False, 'reason': 'not_connected'})
                elif not is_planar:
                    graph_validities.append({'valid': False, 'reason': 'not_planar'})
                elif not is_unique:
                    graph_validities.append({'valid': False, 'reason': 'not_unique'})
                else:
                    graph_validities.append({'valid': True, 'reason': 'valid'})
                    valid_graphs.append(nx_graph)
            
            print(f"   ğŸ“‹ æ‰¹é‡ç¡¬çº¦æŸæ£€æŸ¥å®Œæˆ: {len(valid_graphs)}/{len(nx_graphs)} ä¸ªå›¾æœ‰æ•ˆ")
            
            # === æ­¥éª¤2: æ‰¹é‡è®¡ç®—metricsï¼ˆä»…å¯¹æœ‰æ•ˆå›¾ï¼‰ ===
            if len(valid_graphs) == 0:
                return [0.01] * len(nx_graphs)
            
            batch_metrics = self._compute_batch_evaluation_metrics(valid_graphs)
            batch_avg_ratio = self._compute_avg_ratio(batch_metrics)
            
            print(f"   ğŸ§® æ‰¹é‡metricsè®¡ç®—å®Œæˆï¼Œavg_ratio: {batch_avg_ratio:.4f}")
            
            # === æ­¥éª¤3: ç”Ÿæˆå¥–åŠ±åˆ—è¡¨ ===
            rewards = []
            valid_idx = 0
            
            for validity in graph_validities:
                if not validity['valid']:
                    # æ ¹æ®å¤±è´¥åŸå› ç»™ä¸åŒçš„æƒ©ç½š
                    if validity['reason'] == 'not_planar':
                        rewards.append(0.05)  # å¹³é¢æ€§å¤±è´¥ç»™ç¨é«˜ä¸€ç‚¹çš„å¥–åŠ±
                    else:
                        rewards.append(0.01)  # å…¶ä»–æƒ…å†µç»™æœ€ä½å¥–åŠ±
                else:
                    # åŸºäºbatch avg_ratioè®¡ç®—å¥–åŠ±
                    decay_factor = 0.045
                    reward = np.exp(-decay_factor * batch_avg_ratio)
                    rewards.append(max(0.01, reward))
                    valid_idx += 1
            
            batch_time = time.time() - batch_start
            print(f"   â±ï¸  æ‰¹é‡è®¡ç®—å®Œæˆï¼Œè€—æ—¶ {batch_time:.2f}s")
            
            return rewards
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡è®¡ç®—å¥–åŠ±å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return [0.01] * len(nx_graphs)
    
    def _compute_parallel_rewards(self, nx_graphs: List[nx.Graph]) -> List[float]:
        """å¹¶è¡Œè®¡ç®—å¤šä¸ªå›¾çš„å¥–åŠ±"""
        try:
            if len(nx_graphs) == 0:
                return []
            
            parallel_start = time.time()
            #print(f"   ğŸ”„ å¯åŠ¨ {self.n_workers} ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶è¡Œè®¡ç®—...")
            
            # å‡†å¤‡å‚æ•°ï¼ˆéœ€è¦åºåˆ—åŒ–çš„æ•°æ®ï¼‰
            graph_data = []
            for i, nx_graph in enumerate(nx_graphs):
                # å°†å›¾è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
                graph_dict = {
                    'nodes': list(nx_graph.nodes()),
                    'edges': list(nx_graph.edges()),
                    'graph_idx': i
                }
                graph_data.append(graph_dict)
            
            # å‡†å¤‡å…±äº«çš„å‚è€ƒæ•°æ®
            ref_graphs_data = []
            for ref_graph in self._reference_graphs[:100]:  # ä½¿ç”¨å‰100ä¸ªå‚è€ƒå›¾
                ref_dict = {
                    'nodes': list(ref_graph.nodes()),
                    'edges': list(ref_graph.edges())
                }
                ref_graphs_data.append(ref_dict)
            
            shared_data = {
                'reference_graphs': ref_graphs_data,
                'reference_metrics': self._reference_metrics,
                'recent_graphs': [
                    {'nodes': list(g.nodes()), 'edges': list(g.edges())} 
                    for g in self._recent_generated_graphs[-20:]
                ]
            }
            
            # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œè®¡ç®—
            with ProcessPoolExecutor(max_workers=self.n_workers) as executor:
                # æäº¤ä»»åŠ¡
                future_to_idx = {}
                for i, graph_dict in enumerate(graph_data):
                    future = executor.submit(
                        _compute_single_reward_worker,
                        graph_dict,
                        shared_data,
                        i
                    )
                    future_to_idx[future] = i
                
                # æ”¶é›†ç»“æœ
                rewards = [0.01] * len(nx_graphs)
                completed = 0
                
                for future in as_completed(future_to_idx):
                    idx = future_to_idx[future]
                    try:
                        reward = future.result()
                        rewards[idx] = reward
                        completed += 1
                        
                        if completed % max(1, len(nx_graphs) // 10) == 0:
                            # print(f"   âš¡ è¿›åº¦: {completed}/{len(nx_graphs)}")
                            pass
                    except Exception as e:
                        print(f"   âš ï¸ å›¾ {idx} è®¡ç®—å¤±è´¥: {e}")
                        rewards[idx] = 0.01
            
            parallel_time = time.time() - parallel_start
            #print(f"   â±ï¸  å¹¶è¡Œè®¡ç®—å®Œæˆï¼Œè€—æ—¶ {parallel_time:.2f}s")
            
            return rewards
            
        except Exception as e:
            print(f"âŒ å¹¶è¡Œè®¡ç®—å¥–åŠ±å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return [0.01] * len(nx_graphs)
    
    def _compute_batch_evaluation_metrics(self, nx_graphs: List[nx.Graph]) -> Dict[str, float]:
        """æ‰¹é‡è®¡ç®—è¯„ä¼°æŒ‡æ ‡ - ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰å›¾"""
        try:
            if len(nx_graphs) == 0:
                return {key: 999.0 for key in ['degree', 'clustering', 'orbit', 'spectre', 'wavelet']}
            
            print(f"   ğŸ” æ‰¹é‡è®¡ç®— {len(nx_graphs)} ä¸ªå›¾çš„è¯„ä¼°æŒ‡æ ‡...")
            
            # ä½¿ç”¨å‚è€ƒå›¾çš„å­é›†æ¥åŠ é€Ÿè®¡ç®—
            ref_subset = self._reference_graphs[:100]
            
            metrics = {}
            
            # æ‰¹é‡è®¡ç®—å„é¡¹æŒ‡æ ‡
            metrics['degree'] = self.degree_stats(
                ref_subset,
                nx_graphs,
                is_parallel=True,  # å¯ç”¨å†…éƒ¨å¹¶è¡Œ
                compute_emd=False
            )
            
            metrics['clustering'] = self.clustering_stats(
                ref_subset,
                nx_graphs,
                is_parallel=True,
                compute_emd=False
            )
            
            metrics['orbit'] = self.orbit_stats_all(
                ref_subset,
                nx_graphs,
                compute_emd=False
            )
            
            metrics['spectre'] = self.spectral_stats(
                ref_subset,
                nx_graphs,
                is_parallel=True,
                compute_emd=False
            )
            
            # WaveletæŒ‡æ ‡ï¼ˆåŸºäºspectralçš„å˜ä½“ï¼‰
            metrics['wavelet'] = metrics['spectre'] * 0.85
            
            print(f"   ğŸ“Š æ‰¹é‡metrics: {metrics}")
            return metrics
            
        except Exception as e:
            print(f"âš ï¸ æ‰¹é‡è®¡ç®—è¯„ä¼°æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return {key: 999.0 for key in ['degree', 'clustering', 'orbit', 'spectre', 'wavelet']}

    def _compute_single_graph_reward_strict(self, nx_graph: nx.Graph, graph_idx: int) -> float:
        """ä¸¥æ ¼æŒ‰ç…§è¯„ä¼°æ ‡å‡†è®¡ç®—å•å›¾å¥–åŠ±"""
        try:
            # === æ­¥éª¤1: ç¡¬çº¦æŸæ£€æŸ¥ ===
            if nx_graph.number_of_nodes() == 0:
                return 0.01
            
            # æ£€æŸ¥è¿é€šæ€§å’Œå¹³é¢æ€§
            is_connected = self.nx.is_connected(nx_graph)
            is_planar = self.nx.check_planarity(nx_graph)[0]
            planar_validity = is_connected and is_planar
            
            # æ£€æŸ¥ç‹¬ç‰¹æ€§ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            is_unique = self._check_uniqueness_simple(nx_graph)
            
            # ç¡¬çº¦æŸï¼šå¿…é¡»åŒæ—¶æ»¡è¶³è¿é€šæ€§ã€å¹³é¢æ€§å’Œç‹¬ç‰¹æ€§
            if not is_connected:
                print("è¿é€šæ€§æ£€æŸ¥å¤±è´¥")
                return 0.01  # å·¨å¤§æƒ©ç½š
            if not is_planar:
                print("å¹³é¢æ€§æ£€æŸ¥å¤±è´¥")
                return 0.1  # å·¨å¤§æƒ©ç½š
            if not is_unique:
                print("ç‹¬ç‰¹æ€§æ£€æŸ¥å¤±è´¥")
                return 0.01  # å·¨å¤§æƒ©ç½š
            
            # === æ­¥éª¤2: è®¡ç®—çœŸå®è¯„ä¼°æŒ‡æ ‡ ===
            generated_metrics = self._compute_real_evaluation_metrics([nx_graph])
            
            # === æ­¥éª¤3: è®¡ç®— avg_ratio ===
            avg_ratio = self._compute_avg_ratio(generated_metrics)
            
            # === æ­¥éª¤4: è½¬æ¢ä¸ºå¥–åŠ±åˆ†æ•° ===
            # ä½¿ç”¨æŒ‡æ•°è¡°å‡å‡½æ•°æ¥è®¡ç®—å¥–åŠ±ã€‚avg_ratioè¶Šæ¥è¿‘0ï¼Œå¥–åŠ±è¶Šæ¥è¿‘1ã€‚
            # è¡°å‡ç³»æ•° k çš„é€‰æ‹©ä½¿å¾—åœ¨ avg_ratio=100 æ—¶ï¼Œå¥–åŠ±å·²ç»éå¸¸æ¥è¿‘äº0ã€‚
            decay_factor = 0.045  # k â‰ˆ -ln(0.01) / 100
            
            reward = np.exp(-decay_factor * avg_ratio)
            
            # ç¡®ä¿å¥–åŠ±åœ¨åˆç†èŒƒå›´å†…ï¼Œé¿å…å®Œå…¨ä¸º0
            return max(0.01, reward)
            
        except Exception as e:
            print(f"âš ï¸ è®¡ç®—å›¾ {graph_idx} å¥–åŠ±æ—¶å‡ºé”™: {e}")
            return 0.01
    
    def _compute_real_evaluation_metrics(self, nx_graphs: List) -> Dict[str, float]:
        """è®¡ç®—çœŸå®çš„è¯„ä¼°æŒ‡æ ‡ - ä¸æ¨ç†æ—¶å®Œå…¨ä¸€è‡´"""
        try:
            metrics = {}
            
            # è¿‡æ»¤ç©ºå›¾
            valid_graphs = [g for g in nx_graphs if g.number_of_nodes() > 0]
            if len(valid_graphs) == 0:
                return {key: 999.0 for key in ['degree', 'clustering', 'orbit', 'spectre', 'wavelet']}
            
            # è®¡ç®—å„é¡¹æŒ‡æ ‡ - ä½¿ç”¨çœŸå®çš„è¯„ä¼°å‡½æ•°
            metrics['degree'] = self.degree_stats(
                self._reference_graphs[:100],  # ä½¿ç”¨å‚è€ƒå›¾å­é›†
                valid_graphs,
                is_parallel=False,
                compute_emd=False
            )
            
            metrics['clustering'] = self.clustering_stats(
                self._reference_graphs[:100],
                valid_graphs, 
                is_parallel=False,
                compute_emd=False
            )
            
            metrics['orbit'] = self.orbit_stats_all(
                self._reference_graphs[:100],
                valid_graphs,
                compute_emd=False
            )
            
            metrics['spectre'] = self.spectral_stats(
                self._reference_graphs[:100],
                valid_graphs,
                is_parallel=False,
                compute_emd=False
            )
            
            # WaveletæŒ‡æ ‡ï¼ˆåŸºäºspectralçš„å˜ä½“ï¼‰
            metrics['wavelet'] = metrics['spectre'] * 0.85
            print(f"ç”Ÿæˆçš„å›¾çš„metrics:{metrics}")
            return metrics
            
        except Exception as e:
            print(f"âš ï¸ è®¡ç®—è¯„ä¼°æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            return {key: 999.0 for key in ['degree', 'clustering', 'orbit', 'spectre', 'wavelet']}
    
    def _compute_avg_ratio(self, generated_metrics: Dict[str, float]) -> float:
        """è®¡ç®— avg_ratio - ä¸ metrics/abstract_metrics.py ä¸­ compute_ratios å®Œå…¨ä¸€è‡´"""
        try:
            ratios = []
            metrics_keys = ['degree', 'clustering', 'orbit', 'spectre', 'wavelet']
            
            for key in metrics_keys:
                if key in generated_metrics and key in self._reference_metrics:
                    ref_metric = self._reference_metrics[key]
                    gen_metric = generated_metrics[key]
                    
                    if ref_metric > 1e-8:  # é¿å…é™¤é›¶
                        ratio = gen_metric / ref_metric
                        ratios.append(ratio)
                    else:
                        ratios.append(999.0)  # å‚è€ƒå€¼ä¸º0çš„æƒ…å†µ
            print(f"ratio åˆ†åˆ«æ˜¯ï¼š {ratios}")
            if len(ratios) > 0:
                avg_ratio = sum(ratios) / len(ratios)
            else:
                avg_ratio = 999.0
            
            return avg_ratio
            
        except Exception as e:
            print(f"âš ï¸ è®¡ç®— avg_ratio æ—¶å‡ºé”™: {e}")
            return 999.0
    
    def _check_uniqueness_simple(self, nx_graph: nx.Graph) -> bool:
        """ç®€åŒ–çš„ç‹¬ç‰¹æ€§æ£€æŸ¥"""
        try:
            # ä¸æœ€è¿‘ç”Ÿæˆçš„å›¾è¿›è¡Œæ¯”è¾ƒ
            for recent_graph in self._recent_generated_graphs[-20:]:  # æ£€æŸ¥æœ€è¿‘20ä¸ª
                if (nx_graph.number_of_nodes() == recent_graph.number_of_nodes() and
                    nx_graph.number_of_edges() == recent_graph.number_of_edges()):
                    
                    # åº¦åºåˆ—æ¯”è¾ƒ
                    deg_seq1 = sorted([d for n, d in nx_graph.degree()])
                    deg_seq2 = sorted([d for n, d in recent_graph.degree()])
                    if deg_seq1 == deg_seq2:
                        # è¿›ä¸€æ­¥æ£€æŸ¥èšç±»ç³»æ•°
                        try:
                            clust1 = self.nx.average_clustering(nx_graph)
                            clust2 = self.nx.average_clustering(recent_graph)
                            if abs(clust1 - clust2) < 0.01:
                                return False  # å¯èƒ½é‡å¤
                        except:
                            pass
            
            # æ·»åŠ åˆ°æœ€è¿‘ç”Ÿæˆåˆ—è¡¨
            self._recent_generated_graphs.append(nx_graph.copy())
            if len(self._recent_generated_graphs) > 50:
                self._recent_generated_graphs = self._recent_generated_graphs[-30:]
            
            return True  # ç‹¬ç‰¹
            
        except Exception as e:
            return True  # å‡ºé”™æ—¶å‡è®¾ç‹¬ç‰¹

class IntrinsicQualityReward(BaseRewardFunction):
    """
    å†…åœ¨å“è´¨å¥–åŠ±å‡½æ•° (ä¸“ä¸ºå›ºå®šèŠ‚ç‚¹æ•°å›¾è®¾è®¡)
    [å·²æ›´æ–°ä»¥æ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œè®¡ç®—]
    ... (æ–‡æ¡£å­—ç¬¦ä¸²ä¿æŒä¸å˜) ...
    """
    def __init__(self, datamodule, n_workers: int = 4, device: Optional[torch.device] = None, 
                 weights: Dict[str, float] = None):
        super().__init__("intrinsic_quality", device=device)
        
        self.n_workers = n_workers
        self.parallel_threshold = 4 # å›¾æ•°é‡å°‘äºæ­¤å€¼æ—¶ä¸ä½¿ç”¨å¹¶è¡Œï¼Œé¿å…å¼€é”€
        
        # å€Ÿç”¨PlanarMetricsRewardFunctionçš„è¯„ä¼°ç»„ä»¶
        self._eval_helper = PlanarMetricsRewardFunction(
            datamodule=datamodule, n_workers=n_workers, device=device, batch_compute=False
        )
        self._eval_helper._initialize_reference_data()

        # å®šä¹‰å„å¥–åŠ±ç»„ä»¶çš„æƒé‡
        if weights is None:
            self.weights = {'quality': 0.8, 'distribution': 0.2}
        else:
            self.weights = weights
        
        # å®šä¹‰å†…åœ¨å“è´¨çš„å­æƒé‡
        self.quality_sub_weights = {
            'algebraic_connectivity': 0.4, 'global_efficiency': 0.3, 'modularity': 0.3
        }
            
        print(f"ğŸ† åˆ›å»ºå†…åœ¨å“è´¨å¥–åŠ±å‡½æ•° (æœ€ç»ˆç‰ˆ, å¹¶è¡Œæ•°: {self.n_workers})")
        print(f"   - ä¸»æƒé‡: {self.weights}")
        print(f"   - å“è´¨å­æƒé‡: {self.quality_sub_weights}")
        
        # å‡†å¤‡å…±äº«æ•°æ®ï¼Œåªåºåˆ—åŒ–ä¸€æ¬¡ï¼Œé¿å…é‡å¤å¼€é”€
        self._prepare_shared_data()

    def _prepare_shared_data(self):
        """å‡†å¤‡å¯åºåˆ—åŒ–çš„å…±äº«æ•°æ®ï¼Œä¾›å·¥ä½œè¿›ç¨‹ä½¿ç”¨"""
        print("ğŸ“¦ å‡†å¤‡å¹¶è¡Œè®¡ç®—æ‰€éœ€çš„å…±äº«æ•°æ®...")
        ref_graphs_data = []
        # åªåºåˆ—åŒ–éƒ¨åˆ†å‚è€ƒå›¾ä»¥å‡å°å¼€é”€
        for ref_graph in self._eval_helper._reference_graphs[:100]:
            ref_graphs_data.append({
                'nodes': list(ref_graph.nodes()),
                'edges': list(ref_graph.edges()),
            })

        self.shared_data = {
            'weights': self.weights,
            'quality_sub_weights': self.quality_sub_weights,
            'reference_metrics': self._eval_helper._reference_metrics,
            'reference_graphs_data': ref_graphs_data
        }
        print("âœ… å…±äº«æ•°æ®å‡†å¤‡å®Œæ¯•ã€‚")


    def __call__(self, graphs: List[Tuple[torch.Tensor, torch.Tensor]]) -> torch.Tensor:
        if len(graphs) == 0:
            return torch.tensor([], dtype=torch.float32, device=self.device)

        # æ ¹æ®å›¾çš„æ•°é‡å’Œn_workersè®¾ç½®å†³å®šæ˜¯å¦ä½¿ç”¨å¹¶è¡Œ
        use_parallel = self.n_workers > 1 and len(graphs) >= self.parallel_threshold

        if use_parallel:
            # --- å¹¶è¡Œè®¡ç®—è·¯å¾„ ---
            rewards = self._calculate_parallel(graphs)
        else:
            # --- åºè´¯è®¡ç®—è·¯å¾„ (ç”¨äºè°ƒè¯•æˆ–å¤„ç†å°‘é‡å›¾) ---
            rewards = self._calculate_sequential(graphs)
        
        avg_reward = np.mean(rewards) if rewards else 0
        mode = "å¹¶è¡Œ" if use_parallel else "åºè´¯"
        print(f"ğŸ’ [{mode}æ¨¡å¼] å†…åœ¨å“è´¨å¥–åŠ±è®¡ç®—å®Œæˆ, å¹³å‡å¥–åŠ±: {avg_reward:.4f}")

        return torch.tensor(rewards, dtype=torch.float32, device=self.device)

    def _calculate_sequential(self, graphs: List[Tuple[torch.Tensor, torch.Tensor]]) -> list[float]:
        """åŸå§‹çš„åºè´¯è®¡ç®—æ–¹æ³•"""
        rewards = []
        for i, (atom_types, edge_types) in enumerate(graphs):
            try:
                nx_graph = self._convert_to_networkx(atom_types, edge_types)
                # å¤ç”¨å·¥ä½œå‡½æ•°é€»è¾‘ï¼Œç¡®ä¿ç»“æœä¸€è‡´
                graph_data = {'nodes': list(nx_graph.nodes()), 'edges': list(nx_graph.edges())}
                reward = _intrinsic_quality_worker(graph_data, self.shared_data)
                rewards.append(reward)
            except Exception:
                rewards.append(0.0)
        return rewards

    def _calculate_parallel(self, graphs: List[Tuple[torch.Tensor, torch.Tensor]]) -> list[float]:
        """ä½¿ç”¨ProcessPoolExecutorè¿›è¡Œå¹¶è¡Œè®¡ç®—"""
        rewards = [0.0] * len(graphs)
        with ProcessPoolExecutor(max_workers=self.n_workers) as executor:
            future_to_index = {}
            for i, (atom_types, edge_types) in enumerate(graphs):
                try:
                    nx_graph = self._convert_to_networkx(atom_types, edge_types)
                    graph_data = {'nodes': list(nx_graph.nodes()), 'edges': list(nx_graph.edges())}
                    future = executor.submit(_intrinsic_quality_worker, graph_data, self.shared_data)
                    future_to_index[future] = i
                except Exception:
                    # å¦‚æœå›¾è½¬æ¢å¤±è´¥ï¼Œç›´æ¥èµ‹0å¥–åŠ±
                    rewards[i] = 0.0
            
            for future in as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    reward = future.result()
                    rewards[index] = reward
                except Exception as e:
                    # å¦‚æœå·¥ä½œè¿›ç¨‹å‡ºç°å¼‚å¸¸ï¼Œèµ‹0å¥–åŠ±å¹¶æ‰“å°é”™è¯¯
                    print(f"âŒ å·¥ä½œè¿›ç¨‹åœ¨å¤„ç†å›¾ {index} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    rewards[index] = 0.0
        return rewards

def _intrinsic_quality_worker(graph_data: Dict, shared_data: Dict) -> float:
    """
    ä¸ºIntrinsicQualityRewardè®¾è®¡çš„å¤šè¿›ç¨‹å·¥ä½œå‡½æ•°ã€‚
    è®¡ç®—å•ä¸ªå›¾çš„å†…åœ¨å“è´¨å¥–åŠ±ã€‚

    Args:
        graph_data: åºåˆ—åŒ–çš„å›¾æ•°æ® {'nodes': [...], 'edges': [...]}
        shared_data: åŒ…å«å‚è€ƒæŒ‡æ ‡ã€æƒé‡å’Œå‚è€ƒå›¾çš„å…±äº«æ•°æ®

    Returns:
        è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼
    """
    try:
        # --- é‡æ„å›¾å’Œè·å–æ‰€éœ€æ•°æ® ---
        nx_graph = nx.Graph()
        nx_graph.add_nodes_from(graph_data['nodes'])
        nx_graph.add_edges_from(graph_data['edges'])
        
        weights = shared_data['weights']
        quality_sub_weights = shared_data['quality_sub_weights']
        reference_metrics = shared_data['reference_metrics']

        # --- 1. ç¡¬æ€§é—¨æ§›æ£€æŸ¥ ---
        if nx_graph.number_of_nodes() < 3 or not nx.is_connected(nx_graph):
            return 0.0
        if not nx.check_planarity(nx_graph)[0]:
            return 0.0

        # --- 2. è®¡ç®—å†…åœ¨å“è´¨å¥–åŠ± ---
        
        # a) é²æ£’æ€§: ä»£æ•°è¿é€šåº¦
        try:
            alg_conn = nx.algebraic_connectivity(nx_graph)
            score_alg_conn = 1 / (1 + np.exp(-5 * (alg_conn - 0.5)))
        except Exception:
            score_alg_conn = 0.0

        # b) æ•ˆç‡: å…¨å±€æ•ˆç‡
        try:
            glob_eff = nx.global_efficiency(nx_graph)
            score_glob_eff = 1 / (1 + np.exp(-10 * (glob_eff - 0.4)))
        except Exception:
            score_glob_eff = 0.0
            
        # c) ç»“æ„æ€§: æ¨¡å—åº¦
        try:
            communities = louvain_communities(nx_graph, seed=1)
            modularity = nx.community.modularity(nx_graph, communities)
            score_modularity = 1 / (1 + np.exp(-10 * (modularity - 0.3)))
        except Exception:
            score_modularity = 0.0

        r_quality = (quality_sub_weights['algebraic_connectivity'] * score_alg_conn +
                     quality_sub_weights['global_efficiency'] * score_glob_eff +
                     quality_sub_weights['modularity'] * score_modularity)

        # --- 3. è®¡ç®—åˆ†å¸ƒæ‹Ÿåˆå¥–åŠ± ---
        try:
            # é‡æ„å‚è€ƒå›¾ (åªåœ¨éœ€è¦æ—¶)
            reference_graphs = []
            for ref_dict in shared_data['reference_graphs_data']:
                ref_g = nx.Graph()
                ref_g.add_nodes_from(ref_dict['nodes'])
                ref_g.add_edges_from(ref_dict['edges'])
                reference_graphs.append(ref_g)

            # --- è®¡ç®—çœŸå®è¯„ä¼°æŒ‡æ ‡ ---
            gen_metrics = {}
            valid_graphs = [nx_graph]
            ref_subset = reference_graphs[:100] # ä½¿ç”¨å­é›†åŠ é€Ÿ

            gen_metrics['degree'] = degree_stats(ref_subset, valid_graphs, is_parallel=False, compute_emd=False)
            gen_metrics['clustering'] = clustering_stats(ref_subset, valid_graphs, is_parallel=False, compute_emd=False)
            gen_metrics['orbit'] = orbit_stats_all(ref_subset, valid_graphs, compute_emd=False)
            gen_metrics['spectre'] = spectral_stats(ref_subset, valid_graphs, is_parallel=False, compute_emd=False)
            gen_metrics['wavelet'] = gen_metrics['spectre'] * 0.85

            # --- è®¡ç®— avg_ratio ---
            ratios = []
            for key in ['degree', 'clustering', 'orbit', 'spectre', 'wavelet']:
                if reference_metrics[key] > 1e-8:
                    ratios.append(gen_metrics[key] / reference_metrics[key])
            
            avg_ratio = sum(ratios) / len(ratios) if ratios else 999.0
            r_distribution = np.exp(-0.05 * avg_ratio)

        except Exception:
            r_distribution = 0.1

        # --- 4. ç»„åˆæœ€ç»ˆå¥–åŠ± ---
        total_reward = (weights['quality'] * r_quality +
                        weights['distribution'] * r_distribution)

        return total_reward

    except Exception:
        # ç¡®ä¿ä»»ä½•æœªæ•è·çš„å¼‚å¸¸éƒ½è¿”å›0ï¼Œé¿å…è¿›ç¨‹å´©æºƒ
        return 0.0
    
def _compute_single_reward_worker(graph_dict: Dict, shared_data: Dict, graph_idx: int) -> float:
    """
    å¤šè¿›ç¨‹å·¥ä½œå‡½æ•°ï¼šè®¡ç®—å•ä¸ªå›¾çš„å¥–åŠ±
    
    Args:
        graph_dict: åºåˆ—åŒ–çš„å›¾æ•°æ® {'nodes': [...], 'edges': [...]}
        shared_data: å…±äº«çš„å‚è€ƒæ•°æ®
        graph_idx: å›¾ç´¢å¼•
    
    Returns:
        è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼
    """
    try:
        import networkx as nx
        import numpy as np
        
        # é‡æ„NetworkXå›¾
        nx_graph = nx.Graph()
        nx_graph.add_nodes_from(graph_dict['nodes'])
        nx_graph.add_edges_from(graph_dict['edges'])
        
        # é‡æ„å‚è€ƒå›¾
        reference_graphs = []
        for ref_dict in shared_data['reference_graphs']:
            ref_graph = nx.Graph()
            ref_graph.add_nodes_from(ref_dict['nodes'])
            ref_graph.add_edges_from(ref_dict['edges'])
            reference_graphs.append(ref_graph)
        
        reference_metrics = shared_data['reference_metrics']
        
        # === æ­¥éª¤1: ç¡¬çº¦æŸæ£€æŸ¥ ===
        if nx_graph.number_of_nodes() == 0:
            return 0.01
        
        is_connected = nx.is_connected(nx_graph)
        is_planar = nx.check_planarity(nx_graph)[0]
        
        # ç®€åŒ–çš„ç‹¬ç‰¹æ€§æ£€æŸ¥
        is_unique = _check_uniqueness_worker(nx_graph, shared_data['recent_graphs'])
        
        # ç¡¬çº¦æŸï¼šå¿…é¡»åŒæ—¶æ»¡è¶³è¿é€šæ€§ã€å¹³é¢æ€§å’Œç‹¬ç‰¹æ€§
        if not is_connected or not is_planar or not is_unique:
            print("Graph not vaild.")
            if not is_planar:
                return 0.05  # å¹³é¢æ€§å¤±è´¥ç»™ç¨é«˜å¥–åŠ±
            else:
                return 0.01  # å…¶ä»–æƒ…å†µç»™æœ€ä½å¥–åŠ±
        
        # === æ­¥éª¤2: è®¡ç®—çœŸå®è¯„ä¼°æŒ‡æ ‡ ===
        generated_metrics = _compute_metrics_worker([nx_graph], reference_graphs)

        # === æ­¥éª¤3: è®¡ç®— avg_ratio ===
        avg_ratio = _compute_avg_ratio_worker(generated_metrics, reference_metrics)
        # print(f"generated_metrics: {generated_metrics}, avg_ratio: {avg_ratio}")
        # === æ­¥éª¤4: è½¬æ¢ä¸ºå¥–åŠ±åˆ†æ•° ===
        decay_factor = 0.045
        reward = np.exp(-decay_factor * avg_ratio)
        
        return max(0.01, reward)
        
    except Exception as e:
        print(f"âš ï¸ å·¥ä½œè¿›ç¨‹è®¡ç®—å›¾ {graph_idx} æ—¶å‡ºé”™: {e}")
        return 0.01

def _check_uniqueness_worker(nx_graph, recent_graphs_data):
    """å·¥ä½œè¿›ç¨‹ä¸­çš„ç‹¬ç‰¹æ€§æ£€æŸ¥"""
    try:
        import networkx as nx
        
        for recent_dict in recent_graphs_data:
            # é‡æ„æœ€è¿‘çš„å›¾
            recent_graph = nx.Graph()
            recent_graph.add_nodes_from(recent_dict['nodes'])
            recent_graph.add_edges_from(recent_dict['edges'])
            
            if (nx_graph.number_of_nodes() == recent_graph.number_of_nodes() and
                nx_graph.number_of_edges() == recent_graph.number_of_edges()):
                
                # åº¦åºåˆ—æ¯”è¾ƒ
                deg_seq1 = sorted([d for n, d in nx_graph.degree()])
                deg_seq2 = sorted([d for n, d in recent_graph.degree()])
                if deg_seq1 == deg_seq2:
                    try:
                        clust1 = nx.average_clustering(nx_graph)
                        clust2 = nx.average_clustering(recent_graph)
                        if abs(clust1 - clust2) < 0.01:
                            return False  # å¯èƒ½é‡å¤
                    except:
                        pass
        
        return True  # ç‹¬ç‰¹
        
    except Exception:
        return True  # å‡ºé”™æ—¶å‡è®¾ç‹¬ç‰¹

def _compute_metrics_worker(nx_graphs, reference_graphs):
    """å·¥ä½œè¿›ç¨‹ä¸­çš„metricsè®¡ç®—"""
    try:
        # å¯¼å…¥å¿…è¦çš„è¯„ä¼°å‡½æ•°
        from analysis.spectre_utils import degree_stats, clustering_stats, orbit_stats_all, spectral_stats
        
        metrics = {}
        
        # ä½¿ç”¨å‚è€ƒå›¾çš„å‰50ä¸ªæ¥åŠ é€Ÿ
        ref_subset = reference_graphs[:50]
        
        metrics['degree'] = degree_stats(
            ref_subset, nx_graphs, is_parallel=False, compute_emd=False
        )
        metrics['clustering'] = clustering_stats(
            ref_subset, nx_graphs, is_parallel=False, compute_emd=False
        )
        metrics['orbit'] = orbit_stats_all(
            ref_subset, nx_graphs, compute_emd=False
        )
        metrics['spectre'] = spectral_stats(
            ref_subset, nx_graphs, is_parallel=False, compute_emd=False
        )
        metrics['wavelet'] = metrics['spectre'] * 0.85
        
        return metrics
        
    except Exception as e:
        print(f"âš ï¸ å·¥ä½œè¿›ç¨‹è®¡ç®—metricså¤±è´¥: {e}")
        return {key: 999.0 for key in ['degree', 'clustering', 'orbit', 'spectre', 'wavelet']}

def _compute_avg_ratio_worker(generated_metrics, reference_metrics):
    """å·¥ä½œè¿›ç¨‹ä¸­çš„avg_ratioè®¡ç®—"""
    try:
        ratios = []
        metrics_keys = ['degree', 'clustering', 'orbit', 'spectre', 'wavelet']
        
        for key in metrics_keys:
            if key in generated_metrics and key in reference_metrics:
                ref_metric = reference_metrics[key]
                gen_metric = generated_metrics[key]
                
                if ref_metric > 1e-8:
                    ratio = gen_metric / ref_metric
                    ratios.append(ratio)
                else:
                    ratios.append(999.0)
        
        if len(ratios) > 0:
            return sum(ratios) / len(ratios)
        else:
            return 999.0
            
    except Exception:
        return 999.0


def create_reward_function(
    reward_type: str,
    cfg=None,
    device=None,
    **kwargs
) -> BaseRewardFunction:
    """
    å¥–åŠ±å‡½æ•°å·¥å‚å‡½æ•°
    
    Args:
        reward_type: å¥–åŠ±å‡½æ•°ç±»å‹
        cfg: å®Œæ•´çš„é…ç½®å¯¹è±¡
        device: è®¾å¤‡
        **kwargs: å…¶ä»–å‚æ•°ï¼Œç”¨äºå…¼å®¹æ—§è°ƒç”¨
    
    Returns:
        å¯¹åº”ç±»å‹çš„å¥–åŠ±å‡½æ•°å®ä¾‹
    """
    reward_type = reward_type.lower()
    
    datamodule = kwargs.get('datamodule')
    model = kwargs.get('model')
    ref_metrics = kwargs.get('ref_metrics')
    name = kwargs.get('name')
    n_workers = cfg.train.n_workers if hasattr(cfg, 'train') else None
    
    # æ–°å¢çš„å¹¶è¡Œè®¡ç®—å‚æ•°
    batch_compute = kwargs.get('batch_compute', True)

    if reward_type == "default":
        print("ğŸ“Š åˆ›å»ºé»˜è®¤å¥–åŠ±å‡½æ•°")
        return DefaultRewardFunction(device=device)
    
    elif reward_type == "evaluation_based":
        print("ğŸ“Š åˆ›å»ºåŸºäºè¯„ä¼°çš„å¥–åŠ±å‡½æ•° (ç›´æ¥è°ƒç”¨ sampling_metrics)")
        if model is None:
            raise ValueError("åŸºäºè¯„ä¼°çš„å¥–åŠ±å‡½æ•°éœ€è¦æä¾› model å‚æ•°")
        return EvaluationBasedRewardFunction(
            model=model,
            ref_metrics=ref_metrics,
            name=name,
            device=device
        )
    
    # elif reward_type == "graph_property":
    #     print(f"ğŸ“Š åˆ›å»ºå›¾å±æ€§å¥–åŠ±å‡½æ•° (ç›®æ ‡å¯†åº¦: {target_density})")
    #     return GraphPropertyRewardFunction(target_density=target_density)
    
    elif reward_type == "planar" or reward_type == "planar_metrics":
        print(f"ğŸ“Š åˆ›å»ºå¹³é¢å›¾ä¸“ä¸šå¥–åŠ±å‡½æ•° (è¯„ä¼°å¯¹é½, å¹¶è¡Œ: {n_workers} workers)")
        if datamodule is None:
            print("âš ï¸  è­¦å‘Š: datamoduleä¸ºNoneï¼Œå¹³é¢å›¾å¥–åŠ±å‡½æ•°å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
        return PlanarMetricsRewardFunction(
            datamodule=datamodule,
            n_workers=n_workers,
            batch_compute=batch_compute,
            device=device
        )
    elif reward_type == "intrinsic_quality":
        print("ğŸ“Š åˆ›å»ºå†…åœ¨å“è´¨å¥–åŠ±å‡½æ•° (ä¸“ä¸ºå›ºå®šèŠ‚ç‚¹ã€é«˜è´¨é‡ç”Ÿæˆ)")
        if datamodule is None:
             raise ValueError("å†…åœ¨å“è´¨å¥–åŠ±å‡½æ•°éœ€è¦æä¾› datamodule")
        return IntrinsicQualityReward(
            datamodule=datamodule,
            n_workers=cfg.train.n_workers if hasattr(cfg, 'train') else 4,
            device=device
        )
    
    else:
        print(f"âš ï¸  æœªçŸ¥çš„å¥–åŠ±å‡½æ•°ç±»å‹: {reward_type}ï¼Œä½¿ç”¨é»˜è®¤å¥–åŠ±å‡½æ•°")
        return DefaultRewardFunction(device=device)
